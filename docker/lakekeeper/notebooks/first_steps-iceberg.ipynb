{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T00:56:38.580971Z",
     "start_time": "2025-09-27T00:56:38.360266Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "# This CATALOG_URL works for the \"docker compose\" testing and development environment\n",
    "# Change 'lakekeeper' if you are not running on \"docker compose\" (f. ex. 'localhost' if Lakekeeper is running locally).\n",
    "CATALOG_URL = \"http://lakekeeper:8181/catalog\"\n",
    "WAREHOUSE = \"demo\"\n",
    "\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "SPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\n",
    "ICEBERG_VERSION = \"1.10.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure our Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T00:56:39.334239Z",
     "start_time": "2025-09-27T00:56:39.331042Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    f\"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark.sql.catalog.lakekeeper.type\": \"rest\",\n",
    "    f\"spark.sql.catalog.lakekeeper.uri\": CATALOG_URL,\n",
    "    f\"spark.sql.catalog.lakekeeper.warehouse\": WAREHOUSE,\n",
    "    f\"spark.sql.catalog.lakekeeper.io-impl\": \"org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n",
    "    \"spark.driver.memory\": \"16G\",\n",
    "    \"spark.jars.packages\": f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.13:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T00:56:41.197731Z",
     "start_time": "2025-09-27T00:56:41.191420Z"
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql.shuffle.partitions (need to be smaller for streaming)\n",
    "# looking at the output of the parquet dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T00:56:44.878251Z",
     "start_time": "2025-09-27T00:56:42.241633Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_config = SparkConf().setMaster('local[*]').setAppName(\"Iceberg-REST\")\n",
    "for k, v in config.items():\n",
    "    spark_config = spark_config.set(k, v)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_config).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE lakekeeper\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:56:57.325246Z",
     "start_time": "2025-09-27T00:56:56.548266Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T00:56:58.076495Z",
     "start_time": "2025-09-27T00:56:58.040890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('spark.jars',\n  'file:///home/jovyan/.ivy2.5.2/jars/org.apache.iceberg_iceberg-spark-runtime-4.0_2.13-1.10.0.jar,file:///home/jovyan/.ivy2.5.2/jars/org.apache.iceberg_iceberg-aws-bundle-1.10.0.jar'),\n ('spark.hadoop.fs.s3a.vectored.read.min.seek.size', '128K'),\n ('spark.executor.extraJavaOptions',\n  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true'),\n ('spark.sql.artifact.isolation.enabled', 'false'),\n ('spark.sql.catalog.lakekeeper', 'org.apache.iceberg.spark.SparkCatalog'),\n ('spark.sql.extensions',\n  'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n ('spark.driver.host', '5c0fa20271a2'),\n ('spark.master', 'local[*]'),\n ('spark.app.initial.jar.urls',\n  'spark://5c0fa20271a2:33389/jars/org.apache.iceberg_iceberg-spark-runtime-4.0_2.13-1.10.0.jar,spark://5c0fa20271a2:33389/jars/org.apache.iceberg_iceberg-aws-bundle-1.10.0.jar'),\n ('spark.repl.local.jars',\n  'file:///home/jovyan/.ivy2.5.2/jars/org.apache.iceberg_iceberg-spark-runtime-4.0_2.13-1.10.0.jar,file:///home/jovyan/.ivy2.5.2/jars/org.apache.iceberg_iceberg-aws-bundle-1.10.0.jar'),\n ('spark.app.startTime', '1758934603351'),\n ('spark.executor.id', 'driver'),\n ('spark.driver.port', '33389'),\n ('spark.app.initial.file.urls',\n  'file:///home/jovyan/.ivy2.5.2/jars/org.apache.iceberg_iceberg-spark-runtime-4.0_2.13-1.10.0.jar,file:///home/jovyan/.ivy2.5.2/jars/org.apache.iceberg_iceberg-aws-bundle-1.10.0.jar'),\n ('spark.driver.memory', '16G'),\n ('spark.submit.deployMode', 'client'),\n ('spark.serializer.objectStreamReset', '100'),\n ('spark.files',\n  'file:///home/jovyan/.ivy2.5.2/jars/org.apache.iceberg_iceberg-spark-runtime-4.0_2.13-1.10.0.jar,file:///home/jovyan/.ivy2.5.2/jars/org.apache.iceberg_iceberg-aws-bundle-1.10.0.jar'),\n ('spark.app.submitTime', '1758934603140'),\n ('spark.sql.warehouse.dir', 'file:/home/jovyan/spark-warehouse'),\n ('spark.ui.showConsoleProgress', 'true'),\n ('spark.rdd.compress', 'True'),\n ('spark.sql.catalog.lakekeeper.warehouse', 'demo'),\n ('spark.sql.defaultCatalog', 'lakekeeper'),\n ('spark.sql.catalog.lakekeeper.uri', 'http://lakekeeper:8181/catalog'),\n ('spark.jars.packages',\n  'org.apache.iceberg:iceberg-spark-runtime-4.0_2.13:1.10.0,org.apache.iceberg:iceberg-aws-bundle:1.10.0'),\n ('spark.driver.extraJavaOptions',\n  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true'),\n ('spark.hadoop.fs.s3a.vectored.read.max.merged.size', '2M'),\n ('spark.app.name', 'Iceberg-REST'),\n ('spark.submit.pyFiles',\n  '/home/jovyan/.ivy2.5.2/jars/org.apache.iceberg_iceberg-spark-runtime-4.0_2.13-1.10.0.jar,/home/jovyan/.ivy2.5.2/jars/org.apache.iceberg_iceberg-aws-bundle-1.10.0.jar'),\n ('spark.sql.catalog.lakekeeper.type', 'rest'),\n ('spark.sql.catalog.lakekeeper.io-impl',\n  'org.apache.iceberg.aws.s3.S3FileIO'),\n ('spark.app.id', 'local-1758934603665')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T00:56:59.189925Z",
     "start_time": "2025-09-27T00:56:59.184131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing shuffle partitions to 64\n"
     ]
    }
   ],
   "source": [
    "if int(spark.conf.get(\"spark.sql.shuffle.partitions\")) > 100:\n",
    "    print(\"reducing shuffle partitions to 64\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:00.761116Z",
     "start_time": "2025-09-27T00:57:00.755846Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is a favorite of mine. Turn on zstd to make things highly compressed. Less size on disk, less IO bandwidth!\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:01.463125Z",
     "start_time": "2025-09-27T00:57:01.456545Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType\n",
    "schema = (StructType([\n",
    "    StructField(\"event_time\", StringType(), False),\n",
    "    StructField(\"event_type\", StringType(), False),\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"category_id\", LongType(), False),\n",
    "    StructField(\"category_code\", StringType(), False),\n",
    "    StructField(\"brand\", StringType(), False),\n",
    "    StructField(\"price\", FloatType(), False),\n",
    "    StructField(\"user_id\", IntegerType(), False),\n",
    "    StructField(\"user_session\", StringType(), False),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:02.951466Z",
     "start_time": "2025-09-27T00:57:02.944817Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dataset_dir = Path('/home/jovyan').joinpath('datasets').absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "ecomm_raw_dir = dataset_dir.joinpath('ecomm_raw')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:03.413137Z",
     "start_time": "2025-09-27T00:57:03.409206Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "october_data = (ecomm_raw_dir.joinpath(\"2019-Oct.csv\")).as_posix()\n",
    "november_data = (ecomm_raw_dir.joinpath(\"2019-Nov.csv\")).as_posix()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:03.778826Z",
     "start_time": "2025-09-27T00:57:03.775622Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the E-commerce Data for ingestion to Iceberg\n",
    "> we'll be using the '2019-Oct.csv', '2019-Nov.csv' datasets, doing some minor tweaks and then using these to populate our base Iceberg tables\n",
    "> \n",
    "> Then we'll move on to doing Iceberg Streaming in Part 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "ecomm_oct_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .schema(schema)\n",
    "    .load(october_data)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:05.940351Z",
     "start_time": "2025-09-27T00:57:05.811429Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "|2019-10-01 00:00:...|      view|  44600062|2103807459595387724|                NULL|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|\n",
      "|2019-10-01 00:00:...|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua|   33.2|554748717|9333dfbd-b87a-470...|\n",
      "|2019-10-01 00:00:...|      view|  17200506|2053013559792632471|furniture.living_...|    NULL|  543.1|519107250|566511c2-e2e3-422...|\n",
      "|2019-10-01 00:00:...|      view|   1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|\n",
      "|2019-10-01 00:00:...|      view|   1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|\n",
      "|2019-10-01 00:00:...|      view|   1480613|2053013561092866779|   computers.desktop|  pulser| 908.62|512742880|0d0d91c2-c9c2-4e8...|\n",
      "|2019-10-01 00:00:...|      view|  17300353|2053013553853497655|                NULL|   creed| 380.96|555447699|4fe811e9-91de-46d...|\n",
      "|2019-10-01 00:00:...|      view|  31500053|2053013558031024687|                NULL|luminarc|  41.16|550978835|6280d577-25c8-414...|\n",
      "|2019-10-01 00:00:...|      view|  28719074|2053013565480109009|  apparel.shoes.keds|   baden| 102.71|520571932|ac1cd4e5-a3ce-422...|\n",
      "|2019-10-01 00:00:...|      view|   1004545|2053013555631882655|electronics.smart...|  huawei| 566.01|537918940|406c46ed-90a4-478...|\n",
      "|2019-10-01 00:00:...|      view|   2900536|2053013554776244595|appliances.kitche...|elenberg|  51.46|555158050|b5bdd0b3-4ca2-4c5...|\n",
      "|2019-10-01 00:00:...|      view|   1005011|2053013555631882655|electronics.smart...| samsung| 900.64|530282093|50a293fb-5940-41b...|\n",
      "|2019-10-01 00:00:...|      view|   3900746|2053013552326770905|appliances.enviro...|   haier| 102.38|555444559|98b88fa0-d8fa-4b9...|\n",
      "|2019-10-01 00:00:...|      view|  44600062|2103807459595387724|                NULL|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|\n",
      "|2019-10-01 00:00:...|      view|  13500240|2053013557099889147|furniture.bedroom...|     brw|  93.18|555446365|7f0062d8-ead0-4e0...|\n",
      "|2019-10-01 00:00:...|      view|  23100006|2053013561638126333|                NULL|    NULL| 357.79|513642368|17566c27-0a8f-450...|\n",
      "|2019-10-01 00:00:...|      view|   1801995|2053013554415534427|electronics.video.tv|   haier| 193.03|537192226|e3151795-c355-4ef...|\n",
      "|2019-10-01 00:00:...|      view|  10900029|2053013555069845885|appliances.kitche...|   bosch|  58.95|519528062|901b9e3c-3f8f-414...|\n",
      "|2019-10-01 00:00:...|      view|   1306631|2053013558920217191|  computers.notebook|      hp| 580.89|550050854|7c90fc70-0e80-459...|\n",
      "|2019-10-01 00:00:...|      view|   1005135|2053013555631882655|electronics.smart...|   apple|1747.79|535871217|c6bd7419-2748-4c5...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "ecomm_oct_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:07.375623Z",
     "start_time": "2025-09-27T00:57:06.745762Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "42448764"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecomm_oct_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:11.693305Z",
     "start_time": "2025-09-27T00:57:09.247466Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "ecomm_nov_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .schema(schema)\n",
    "    .load(november_data)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:14.899639Z",
     "start_time": "2025-09-27T00:57:14.874807Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|   brand| price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "|2019-11-01 00:00:...|      view|   1003461|2053013555631882655|electronics.smart...|  xiaomi|489.07|520088904|4d3b30da-a5e4-49d...|\n",
      "|2019-11-01 00:00:...|      view|   5000088|2053013566100866035|appliances.sewing...|  janome|293.65|530496790|8e5f4f83-366c-4f7...|\n",
      "|2019-11-01 00:00:...|      view|  17302664|2053013553853497655|                NULL|   creed| 28.31|561587266|755422e7-9040-477...|\n",
      "|2019-11-01 00:00:...|      view|   3601530|2053013563810775923|appliances.kitche...|      lg|712.87|518085591|3bfb58cd-7892-48c...|\n",
      "|2019-11-01 00:00:...|      view|   1004775|2053013555631882655|electronics.smart...|  xiaomi|183.27|558856683|313628f1-68b8-460...|\n",
      "|2019-11-01 00:00:...|      view|   1306894|2053013558920217191|  computers.notebook|      hp|360.09|520772685|816a59f3-f5ae-4cc...|\n",
      "|2019-11-01 00:00:...|      view|   1306421|2053013558920217191|  computers.notebook|      hp|514.56|514028527|df8184cc-3694-454...|\n",
      "|2019-11-01 00:00:...|      view|  15900065|2053013558190408249|                NULL| rondell| 30.86|518574284|5e6ef132-4d7c-473...|\n",
      "|2019-11-01 00:00:...|      view|  12708937|2053013553559896355|                NULL|michelin| 72.72|532364121|0a899268-31eb-46d...|\n",
      "|2019-11-01 00:00:...|      view|   1004258|2053013555631882655|electronics.smart...|   apple|732.07|532647354|d2d3d2c6-631d-489...|\n",
      "|2019-11-01 00:00:...|      view|  17200570|2053013559792632471|furniture.living_...|    NULL|437.33|518780843|aa806835-b14c-45a...|\n",
      "|2019-11-01 00:00:...|      view|   2701517|2053013563911439225|appliances.kitche...|    NULL|155.11|518427361|c89b0d96-247f-404...|\n",
      "|2019-11-01 00:00:...|      view|  16700260|2053013559901684381|furniture.kitchen...|    NULL| 31.64|566255262|173d7b72-1db7-463...|\n",
      "|2019-11-01 00:00:...|      view|  34600011|2060981320581906480|                NULL|    NULL| 20.54|512416379|4dfe2c67-e537-4dc...|\n",
      "|2019-11-01 00:00:...|      view|   4600658|2053013563944993659|appliances.kitche...| samsung|411.83|526595547|aab33a9a-29c3-4d5...|\n",
      "|2019-11-01 00:00:...|      view|  24900193|2053013562183385881|                NULL|    NULL|  1.09|512651494|f603c815-f51a-46f...|\n",
      "|2019-11-01 00:00:...|      view|  27400066|2053013563391345499|                NULL|    NULL|  8.55|551061950|3f6112f1-5695-4e8...|\n",
      "|2019-11-01 00:00:...|      view|   5100503|2053013553375346967|                NULL|  xiaomi| 22.68|520037415|f54fa96a-f3f2-43a...|\n",
      "|2019-11-01 00:00:...|      view|   1004566|2053013555631882655|electronics.smart...|  huawei|164.84|566265908|52c2c76c-b79e-479...|\n",
      "|2019-11-01 00:00:...|      view|   1307115|2053013558920217191|  computers.notebook|      hp|411.59|514028527|df8184cc-3694-454...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "ecomm_nov_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:15.537057Z",
     "start_time": "2025-09-27T00:57:15.460248Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "67501979"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecomm_nov_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:21.334627Z",
     "start_time": "2025-09-27T00:57:18.130877Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the Initial Parquet Tables \n",
    "> These will live outside of MinIO and the Iceberg Catalog\n",
    "\n",
    "* We'll create a simple helper function to _aid_ in our journey. We'll call it `write_parquet`.\n",
    "* Using the new _function_, we'll then need to convert the **csv**->**parquet**, so run the **two** cells under \"convert the data\". They will produce raw parquet records under the path `datasets/ecomm_raw/parquet/ecomm`.\n",
    "* We will then be able to run a series of 'daily' transactions to **write** all of the records into our foundational **iceberg** table."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def write_parquet(df: DataFrame, destination: Path, sink_dir: str) -> DataFrame:\n",
    "    save_path = destination.joinpath('parquet', sink_dir)\n",
    "    # modifications to the dataframe\n",
    "    transformed = (\n",
    "        df\n",
    "            .withColumn(\"event_time\", to_timestamp(col(\"event_time\"), \"yyyy-MM-dd HH:mm:ss z\"))\n",
    "            .withColumn(\"event_date\", to_date(col(\"event_time\")))\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        transformed\n",
    "            .write\n",
    "            .format(\"parquet\")\n",
    "            .partitionBy(\"event_date\")\n",
    "            .mode(\"append\")\n",
    "            .save(save_path.as_posix())\n",
    "    )\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:52:50.860794Z",
     "start_time": "2025-09-27T00:52:50.846383Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert the Data\n",
    "We're on a mission to convert CSV to Parquet. Let's do that next.\n",
    "> Note: This process may take a while if you're using the full dataset (~19GB)\n",
    "> Note: The function \"expects\" that we'll be _appending__ to the **ecomm** directory.\n",
    "> * If you want to modify the behavior of the function, give it a new argument called mode, and default that to \"errorIfExists\" of \"ignore\" - so you don't have to worry about deduplication or going nuculear and wiping out the entire ecomm directory!\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh good. Saved you from having to import again\n"
     ]
    }
   ],
   "source": [
    "# note: ecomm_raw_dir is the Path instance to the datasets/ecomm_raw directory\n",
    "# further note: this could take a few minutes locally. Just turn up your favorite jams and let it ride\n",
    "# lastly, the safe guard is on to check if the parquet directory already exists. This is to save you from accidentally \n",
    "# running the import more than once. This is to save you the trouble this can cause.\n",
    "\n",
    "if not ecomm_raw_dir.joinpath('parquet', 'ecomm', 'event_date=2019-10-01').is_dir():\n",
    "  write_parquet(ecomm_oct_df, ecomm_raw_dir, 'ecomm')\n",
    "else:\n",
    "  print(\"oh good. Saved you from having to import again\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:52:56.711465Z",
     "start_time": "2025-09-27T00:52:56.701705Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same goes for the november set. It exists, so we'll skip for now\n"
     ]
    }
   ],
   "source": [
    "if not ecomm_raw_dir.joinpath('parquet', 'ecomm', 'event_date=2019-11-01').is_dir():\n",
    "  write_parquet(ecomm_nov_df, ecomm_raw_dir, 'ecomm')\n",
    "else:\n",
    "  print(\"same goes for the november set. It exists, so we'll skip for now\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:52:58.484193Z",
     "start_time": "2025-09-27T00:52:58.478133Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Iceberg: Creating and Appending data to Iceberg Tables\n",
    "> This section will help us build a firm understanding of how to work with Iceberg Tables\n",
    "1. Learn to easily check for table existence\n",
    "2. Learn to use Spark SQL to create Namespaces (this is where our tables live)\n",
    "3. Use the simple table existence check to either a) create a new Iceberg table, or b) append new rows to an existing table within a Namespace"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# let's identify a helper function to check to see if a \"table\" exists\n",
    "# this can be done a few different ways, but this one uses the 'underlying' spark.catalog\n",
    "# and I like that better :)\n",
    "\n",
    "def table_exists(table_name: str):\n",
    "    return any(table.name == table_name for table in spark.catalog.listTables())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:30.359966Z",
     "start_time": "2025-09-27T00:57:30.344104Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tables live in Namespaces\n",
    "Namespaces are a way of **grouping** tables together in the same way you would use **data domains** to separate different categories of data. \n",
    "> In practice, a namespace could be something like \"consumer\", \"product\", \"orders\" for ecommerce.\n",
    "> For our demo, we'll just call the namespace `icystreams` since we are grouping data to use for _Streaming Iceberg_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| namespace|\n",
      "+----------+\n",
      "|icystreams|\n",
      "+----------+\n"
     ]
    }
   ],
   "source": [
    "catalog_namespace = 'icystreams'\n",
    "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {catalog_namespace}\")\n",
    "spark.sql(\"SHOW NAMESPACES\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:32.629062Z",
     "start_time": "2025-09-27T00:57:32.518352Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'lakekeeper'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The current catalog will be the value of the following configuration: spark.conf.get(\"spark.sql.defaultCatalog\")\n",
    "spark.catalog.currentCatalog()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:35.085613Z",
     "start_time": "2025-09-27T00:57:35.076635Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "[Database(name='icystreams', catalog='lakekeeper', description=None, locationUri='s3://examples/initial-warehouse/0199881f-dbaa-7841-9c60-cae98e839ecb')]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# within the Apache Spark ecosystem. Our namespace is equivalent to a traditional hive \"database\", or Unity Catalog \"schema\"\n",
    "spark.catalog.listDatabases()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:36.028141Z",
     "start_time": "2025-09-27T00:57:35.889002Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "[Database(name='icystreams', catalog='lakekeeper', description=None, locationUri='s3://examples/initial-warehouse/0199881f-dbaa-7841-9c60-cae98e839ecb')]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:37.363701Z",
     "start_time": "2025-09-27T00:57:37.298355Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Note: To simplify how we write to Iceberg, we are going to use the `spark.catalog.*` functions to point to `lakekeeper.icystreams`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase('icystreams')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:40.213671Z",
     "start_time": "2025-09-27T00:57:40.202540Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def write_iceberg(df: DataFrame, namespace: str, table_name: str, partition_col: str) -> DataFrame:\n",
    "    # simplifies the process of `testing` for the state of a given Iceberg table\n",
    "    to_iceberg = (\n",
    "        df\n",
    "            .writeTo(f\"{namespace}.{table_name}\")\n",
    "            .partitionedBy(partition_col)\n",
    "    )\n",
    "    # decided to append or create\n",
    "    if table_exists(table_name):\n",
    "        return to_iceberg.append()\n",
    "    return to_iceberg.create()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-26T22:53:05.741377Z",
     "start_time": "2025-09-26T22:53:05.735195Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# setting this outside of the context of the next **write_iceberg** block\n",
    "# this lets us skip writing if we've already done so\n",
    "iceberg_table_name = 'ecomm'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:44.070315Z",
     "start_time": "2025-09-27T00:57:44.062388Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Writing our Parquet into Iceberg\n",
    "The following cell needs to be re-run for every day of October and November 2019.\n",
    "\n",
    "You can choose to group many days into single transactions\n",
    "```python\n",
    "(spark\n",
    "    .read\n",
    "    .format('parquet')\n",
    "    .load(parquet_dir.as_posix())\n",
    "    .where(\n",
    "        col(\"event_date\").isin(\n",
    "            \"2019-10-01\", \"2019-10-02\", \"2019-10-03\", \"2019-10-04\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "Maybe you want to regroup the data by hour and test creating 24 transactions per day across all days. Really, the world is your oyster!\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# set the reference to the parquet directory\n",
    "parquet_dir = ecomm_raw_dir.joinpath('parquet', 'ecomm')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:48.291139Z",
     "start_time": "2025-09-27T00:57:48.286415Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Run the following block to write a single transaction per day\n",
    "# Note: This took around 4 minutes on a high-powered M3 Ultra computer - non-clustered (eg: spark.master=local[*])\n",
    "# This can run on a single machine really easily - but blindingly fast in a cluster (only latency is IO between your object storage and your Iceberg Rest Catalog API and the Spark Cluster)\n",
    "year = \"2019\"\n",
    "months = ['10', '11']\n",
    "for month in list(months):\n",
    "    for day in (range(1, 32)) if month == '10' else (range(1, 31)):\n",
    "        insert_date = f\"{year}-{month}-0{day}\" if day < 10 else f\"{year}-{month}-{day}\"\n",
    "        write_iceberg(\n",
    "            (spark\n",
    "                .read\n",
    "                .format('parquet')\n",
    "                .load(parquet_dir.as_posix())\n",
    "                .where(col(\"event_date\").isin(insert_date))\n",
    "            ),\n",
    "        catalog_namespace,\n",
    "        iceberg_table_name,\n",
    "        'event_date'\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-26T23:08:57.635255Z",
     "start_time": "2025-09-26T23:04:41.653770Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "       total\n0  109950743",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>109950743</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"select count(*) as total from {catalog_namespace}.{iceberg_table_name}\").toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:55.189719Z",
     "start_time": "2025-09-27T00:57:54.349194Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T00:58:15.226141Z",
     "start_time": "2025-09-27T00:58:13.033303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            event_time event_type  product_id          category_id  \\\n0  2019-11-30 09:44:10       view     1004158  2053013555631882655   \n1  2019-11-30 09:44:10       cart    17300768  2053013553853497655   \n2  2019-11-30 09:44:10       view    12703494  2053013553559896355   \n3  2019-11-30 09:44:10       view     5300252  2053013563173241677   \n4  2019-11-30 09:44:10       view     1002544  2053013555631882655   \n5  2019-11-30 09:44:10       view     1004833  2053013555631882655   \n6  2019-11-30 09:44:10       view     1004743  2053013555631882655   \n7  2019-11-30 09:44:10       cart     1005100  2053013555631882655   \n8  2019-11-30 09:44:10   purchase    12500508  2053013556277805513   \n9  2019-11-30 09:44:10       view    18000951  2053013558525952589   \n10 2019-11-30 09:44:10       view    12702956  2053013553559896355   \n11 2019-11-30 09:44:10       view     1004833  2053013555631882655   \n12 2019-11-30 09:44:10       view    32900155  2055156924407612189   \n13 2019-11-30 09:44:10       view     1307541  2053013558920217191   \n14 2019-11-30 09:44:10       view     1480682  2053013561092866779   \n\n             category_code     brand       price    user_id  \\\n0   electronics.smartphone   samsung  682.130005  575219697   \n1                     None      None   63.750000  519636344   \n2                     None  cordiant   41.189999  515671054   \n3                     None   polaris   25.709999  579548460   \n4   electronics.smartphone     apple  460.500000  513983510   \n5   electronics.smartphone   samsung  167.029999  573759994   \n6   electronics.smartphone    xiaomi   72.050003  516587653   \n7   electronics.smartphone   samsung  131.770004  552718105   \n8                     None      None   37.040001  514037703   \n9                     None   samsung    6.500000  514704543   \n10                    None    nokian   49.939999  538077530   \n11  electronics.smartphone   samsung  167.029999  514872316   \n12         accessories.bag    helios   21.350000  512715261   \n13      computers.notebook      asus  489.359985  531477761   \n14       computers.desktop      acer  371.440002  559512811   \n\n                            user_session  event_date  \n0   d5a319f8-aca2-412c-a797-8e78ba09159f  2019-11-30  \n1   34466b6b-aa50-4571-b327-bcbc015d652e  2019-11-30  \n2   382a1e92-e118-4efb-aa4a-adeafd1d3936  2019-11-30  \n3   2f88f349-f84d-4d1d-bd44-79f7dcb35b7f  2019-11-30  \n4   69ba2f17-993f-4ddf-899d-41d09d1ecd98  2019-11-30  \n5   170b6cd3-8554-45bd-9f74-afd844d6c0bb  2019-11-30  \n6   da277c8c-6b18-4dfe-8bb4-1217ad6b90f1  2019-11-30  \n7   3f781d61-4880-49b6-87f1-c76b8a672781  2019-11-30  \n8   03838da1-e669-4ddd-929f-80ed83f7655c  2019-11-30  \n9   62f805df-084f-4495-96e5-b19b30383f7b  2019-11-30  \n10  fc34faf4-5d7b-4619-956e-da5cb363d2f5  2019-11-30  \n11  f98319cb-150a-4881-9f75-225efc35f283  2019-11-30  \n12  8513665f-e4d5-41aa-9874-eb81a75246e0  2019-11-30  \n13  d531ac89-dbb0-4456-9fef-dde007e3fa55  2019-11-30  \n14  c04dbd7f-99d7-4244-9beb-b5d5a4af7e32  2019-11-30  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_time</th>\n      <th>event_type</th>\n      <th>product_id</th>\n      <th>category_id</th>\n      <th>category_code</th>\n      <th>brand</th>\n      <th>price</th>\n      <th>user_id</th>\n      <th>user_session</th>\n      <th>event_date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>view</td>\n      <td>1004158</td>\n      <td>2053013555631882655</td>\n      <td>electronics.smartphone</td>\n      <td>samsung</td>\n      <td>682.130005</td>\n      <td>575219697</td>\n      <td>d5a319f8-aca2-412c-a797-8e78ba09159f</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>cart</td>\n      <td>17300768</td>\n      <td>2053013553853497655</td>\n      <td>None</td>\n      <td>None</td>\n      <td>63.750000</td>\n      <td>519636344</td>\n      <td>34466b6b-aa50-4571-b327-bcbc015d652e</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>view</td>\n      <td>12703494</td>\n      <td>2053013553559896355</td>\n      <td>None</td>\n      <td>cordiant</td>\n      <td>41.189999</td>\n      <td>515671054</td>\n      <td>382a1e92-e118-4efb-aa4a-adeafd1d3936</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>view</td>\n      <td>5300252</td>\n      <td>2053013563173241677</td>\n      <td>None</td>\n      <td>polaris</td>\n      <td>25.709999</td>\n      <td>579548460</td>\n      <td>2f88f349-f84d-4d1d-bd44-79f7dcb35b7f</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>view</td>\n      <td>1002544</td>\n      <td>2053013555631882655</td>\n      <td>electronics.smartphone</td>\n      <td>apple</td>\n      <td>460.500000</td>\n      <td>513983510</td>\n      <td>69ba2f17-993f-4ddf-899d-41d09d1ecd98</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>view</td>\n      <td>1004833</td>\n      <td>2053013555631882655</td>\n      <td>electronics.smartphone</td>\n      <td>samsung</td>\n      <td>167.029999</td>\n      <td>573759994</td>\n      <td>170b6cd3-8554-45bd-9f74-afd844d6c0bb</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>view</td>\n      <td>1004743</td>\n      <td>2053013555631882655</td>\n      <td>electronics.smartphone</td>\n      <td>xiaomi</td>\n      <td>72.050003</td>\n      <td>516587653</td>\n      <td>da277c8c-6b18-4dfe-8bb4-1217ad6b90f1</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>cart</td>\n      <td>1005100</td>\n      <td>2053013555631882655</td>\n      <td>electronics.smartphone</td>\n      <td>samsung</td>\n      <td>131.770004</td>\n      <td>552718105</td>\n      <td>3f781d61-4880-49b6-87f1-c76b8a672781</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>purchase</td>\n      <td>12500508</td>\n      <td>2053013556277805513</td>\n      <td>None</td>\n      <td>None</td>\n      <td>37.040001</td>\n      <td>514037703</td>\n      <td>03838da1-e669-4ddd-929f-80ed83f7655c</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>view</td>\n      <td>18000951</td>\n      <td>2053013558525952589</td>\n      <td>None</td>\n      <td>samsung</td>\n      <td>6.500000</td>\n      <td>514704543</td>\n      <td>62f805df-084f-4495-96e5-b19b30383f7b</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>view</td>\n      <td>12702956</td>\n      <td>2053013553559896355</td>\n      <td>None</td>\n      <td>nokian</td>\n      <td>49.939999</td>\n      <td>538077530</td>\n      <td>fc34faf4-5d7b-4619-956e-da5cb363d2f5</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>view</td>\n      <td>1004833</td>\n      <td>2053013555631882655</td>\n      <td>electronics.smartphone</td>\n      <td>samsung</td>\n      <td>167.029999</td>\n      <td>514872316</td>\n      <td>f98319cb-150a-4881-9f75-225efc35f283</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>view</td>\n      <td>32900155</td>\n      <td>2055156924407612189</td>\n      <td>accessories.bag</td>\n      <td>helios</td>\n      <td>21.350000</td>\n      <td>512715261</td>\n      <td>8513665f-e4d5-41aa-9874-eb81a75246e0</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>view</td>\n      <td>1307541</td>\n      <td>2053013558920217191</td>\n      <td>computers.notebook</td>\n      <td>asus</td>\n      <td>489.359985</td>\n      <td>531477761</td>\n      <td>d531ac89-dbb0-4456-9fef-dde007e3fa55</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2019-11-30 09:44:10</td>\n      <td>view</td>\n      <td>1480682</td>\n      <td>2053013561092866779</td>\n      <td>computers.desktop</td>\n      <td>acer</td>\n      <td>371.440002</td>\n      <td>559512811</td>\n      <td>c04dbd7f-99d7-4244-9beb-b5d5a4af7e32</td>\n      <td>2019-11-30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "select * from {catalog_namespace}.{iceberg_table_name} \n",
    "where event_date BETWEEN DATE(\"2019-11-15\") AND DATE(\"2019-11-30\")\n",
    "ORDER BY event_date DESC\n",
    "LIMIT 15\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Is the Query Slow or is it Just your Laptop?\n",
    "So far, we've created 61 transactions (the sequence number in the metadata will be 61 if you've run this notebook once). We haven't optimized the table at this point. So we are going to do that now and see if we can reduce the query time. Whether this is a local experiment or production, you'll still need to periodically \"clean\" your tables up!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "#result_df = spark.sql(f\"\"\"\n",
    "#CALL system.rewrite_data_files(\n",
    "#    table => 'lakekeeper.{catalog_namespace}.{iceberg_table_name}',\n",
    "#        options => map(\n",
    "#            'target-file-size-bytes', '134217728', -- 128MB target size\n",
    "#            'min-input-files', '5' -- Minimum files to trigger rewrite\n",
    "#        )\n",
    "#    );\n",
    "#\"\"\")\n",
    "\n",
    "# result_df = spark.sql(f\"CALL system.rewrite_data_files(table => 'lakekeeper.{catalog_namespace}.{iceberg_table_name}')\")\n",
    "\n",
    "# expire snapshots and remove manifest lists\n",
    "result_df = spark.sql(f\"\"\"\n",
    "CALL system.expire_snapshots(\n",
    "  table => 'lakekeeper.{catalog_namespace}.{iceberg_table_name}', \n",
    "  older_than => '2025-09-27 00:00:00'\n",
    ")\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:59:16.414432Z",
     "start_time": "2025-09-27T00:59:15.536405Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " deleted_data_files_count            | 0   \n",
      " deleted_position_delete_files_count | 0   \n",
      " deleted_equality_delete_files_count | 0   \n",
      " deleted_manifest_files_count        | 0   \n",
      " deleted_manifest_lists_count        | 0   \n",
      " deleted_statistics_files_count      | 0   \n"
     ]
    }
   ],
   "source": [
    "result_df.show(1, 0, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T00:59:18.777330Z",
     "start_time": "2025-09-27T00:59:18.745930Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fruits of our Labor?\n",
    "Go ahead and **run the SQL query from before we tried to optimize the table** again. You may notice we're not getting a boost in speed. So what is happening here? \n",
    "\n",
    "This is an issue with the \"partitions\", (oh that again you say!). We can use something called \"hidden partitioning\" here to remove the \"explicit\" directories, after all we are no longer living in a Hive/Hadoop world. Rid yourself of the baggage of \"physical\" directory based partitioning. You'll thank yourself."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# > Note: This fails with S3 IO.\n",
    "#clear_orphans_df = spark.sql(f\"\"\"\n",
    "#CALL system.remove_orphan_files(\n",
    "#  table => 'lakekeeper.{catalog_namespace}.{iceberg_table_name}',\n",
    "#  prefix_listing => false\n",
    "#)\n",
    "#\"\"\")\n",
    "# clear_orphans_df.show(1, 0, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T01:00:08.615422Z",
     "start_time": "2025-09-27T01:00:08.607162Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T23:04:19.599810Z",
     "start_time": "2025-09-26T23:04:19.552554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# > Note: This is the nuclear option to scrap the table and all metadata and simply begin again!\n",
    "# spark.sql(f\"DROP TABLE {catalog_namespace}.{iceberg_table_name}\")\n",
    "#spark.sql(f\"DROP NAMESPACE {catalog_namespace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
