{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T19:41:28.296177Z",
     "start_time": "2025-09-27T19:41:28.060932Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "# This CATALOG_URL works for the \"docker compose\" testing and development environment\n",
    "# Change 'lakekeeper' if you are not running on \"docker compose\" (f. ex. 'localhost' if Lakekeeper is running locally).\n",
    "CATALOG_URL = \"http://lakekeeper:8181/catalog\"\n",
    "WAREHOUSE = \"demo\"\n",
    "\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "SPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\n",
    "ICEBERG_VERSION = \"1.10.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure our Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T19:41:32.155148Z",
     "start_time": "2025-09-27T19:41:32.148494Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n",
    "    \"spark.sql.catalog.lakekeeper.uri\": CATALOG_URL,\n",
    "    \"spark.sql.catalog.lakekeeper.warehouse\": WAREHOUSE,\n",
    "    \"spark.sql.catalog.lakekeeper.io-impl\": \"org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n",
    "    \"spark.driver.memory\": \"24G\",\n",
    "    \"spark.jars.packages\": f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.13:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION}\",\n",
    "    # about zstd: Turn on zstd to make things highly compressed. Less size on disk, less IO bandwidth!\n",
    "    # we want to use zstd for parquet: \n",
    "    \"spark.sql.parquet.compression.codec\": \"zstd\",\n",
    "    \"spark.sql.iceberg.planning.preserve-data-grouping\": \"true\",\n",
    "    # we also want to use zstd for iceberg datafiles\n",
    "    \"spark.sql.iceberg.compression-codec\": \"zstd\",\n",
    "    \"spark.sql.iceberg.locality.enabled\": \"true\",\n",
    "    # note: merge-schema should be only set to true if you \"trust\" the upstream data producer\n",
    "    \"spark.sql.iceberg.merge-schema\": \"false\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T19:41:43.555148Z",
     "start_time": "2025-09-27T19:41:35.492584Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_config = SparkConf().setMaster('local[*]').setAppName(\"Iceberg-REST\")\n",
    "for k, v in config.items():\n",
    "    spark_config = spark_config.set(k, v)\n",
    "\n",
    "spark: SparkSession = SparkSession.builder.config(conf=spark_config).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:41:43.559214Z",
     "start_time": "2025-09-27T19:41:43.555782Z"
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql.shuffle.partitions (need to be smaller for streaming)\n",
    "# the default is 200, which is great for huge data, but bad for small incremental streams\n",
    "# spark.conf.get('spark.sql.shuffle.partitions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:41:44.279110Z",
     "start_time": "2025-09-27T19:41:43.557788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE lakekeeper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T19:41:44.285024Z",
     "start_time": "2025-09-27T19:41:44.278704Z"
    }
   },
   "outputs": [],
   "source": [
    "# uncomment the following to view the full SparkConf\n",
    "# spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T21:33:59.292091Z",
     "start_time": "2025-09-27T21:33:59.217381Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mint\u001B[39m(\u001B[43mspark\u001B[49m.conf.get(\u001B[33m\"\u001B[39m\u001B[33mspark.sql.shuffle.partitions\u001B[39m\u001B[33m\"\u001B[39m)) > \u001B[32m100\u001B[39m:\n\u001B[32m      2\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mreducing shuffle partitions to 32\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      3\u001B[39m     spark.conf.set(\u001B[33m\"\u001B[39m\u001B[33mspark.sql.shuffle.partitions\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m32\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "if int(spark.conf.get(\"spark.sql.shuffle.partitions\")) > 100:\n",
    "    print(\"reducing shuffle partitions to 32\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T19:41:51.961289Z",
     "start_time": "2025-09-27T19:41:51.955266Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType\n",
    "schema = (StructType([\n",
    "    StructField(\"event_time\", StringType(), False),\n",
    "    StructField(\"event_type\", StringType(), False),\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"category_id\", LongType(), False),\n",
    "    StructField(\"category_code\", StringType(), False),\n",
    "    StructField(\"brand\", StringType(), False),\n",
    "    StructField(\"price\", FloatType(), False),\n",
    "    StructField(\"user_id\", IntegerType(), False),\n",
    "    StructField(\"user_session\", StringType(), False),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T19:41:52.699958Z",
     "start_time": "2025-09-27T19:41:52.692512Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dataset_dir = Path('/home/jovyan').joinpath('datasets').absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:41:53.284834Z",
     "start_time": "2025-09-27T19:41:53.280348Z"
    }
   },
   "outputs": [],
   "source": [
    "ecomm_raw_dir = dataset_dir.joinpath('ecomm_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:41:53.665896Z",
     "start_time": "2025-09-27T19:41:53.657942Z"
    }
   },
   "outputs": [],
   "source": [
    "october_data = (ecomm_raw_dir.joinpath(\"2019-Oct.csv\")).as_posix()\n",
    "november_data = (ecomm_raw_dir.joinpath(\"2019-Nov.csv\")).as_posix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create the E-commerce Data for ingestion to Iceberg\n",
    "> we'll be using the '2019-Oct.csv', '2019-Nov.csv' datasets, doing some minor tweaks and then using these to populate our base Iceberg tables\n",
    "> \n",
    "> Then we'll move on to doing Iceberg Streaming in Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:41:55.917432Z",
     "start_time": "2025-09-27T19:41:55.786563Z"
    }
   },
   "outputs": [],
   "source": [
    "ecomm_oct_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .schema(schema)\n",
    "    .load(october_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:41:57.348974Z",
     "start_time": "2025-09-27T19:41:56.696712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "|2019-10-01 00:00:...|      view|  44600062|2103807459595387724|                NULL|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|\n",
      "|2019-10-01 00:00:...|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua|   33.2|554748717|9333dfbd-b87a-470...|\n",
      "|2019-10-01 00:00:...|      view|  17200506|2053013559792632471|furniture.living_...|    NULL|  543.1|519107250|566511c2-e2e3-422...|\n",
      "|2019-10-01 00:00:...|      view|   1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|\n",
      "|2019-10-01 00:00:...|      view|   1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|\n",
      "|2019-10-01 00:00:...|      view|   1480613|2053013561092866779|   computers.desktop|  pulser| 908.62|512742880|0d0d91c2-c9c2-4e8...|\n",
      "|2019-10-01 00:00:...|      view|  17300353|2053013553853497655|                NULL|   creed| 380.96|555447699|4fe811e9-91de-46d...|\n",
      "|2019-10-01 00:00:...|      view|  31500053|2053013558031024687|                NULL|luminarc|  41.16|550978835|6280d577-25c8-414...|\n",
      "|2019-10-01 00:00:...|      view|  28719074|2053013565480109009|  apparel.shoes.keds|   baden| 102.71|520571932|ac1cd4e5-a3ce-422...|\n",
      "|2019-10-01 00:00:...|      view|   1004545|2053013555631882655|electronics.smart...|  huawei| 566.01|537918940|406c46ed-90a4-478...|\n",
      "|2019-10-01 00:00:...|      view|   2900536|2053013554776244595|appliances.kitche...|elenberg|  51.46|555158050|b5bdd0b3-4ca2-4c5...|\n",
      "|2019-10-01 00:00:...|      view|   1005011|2053013555631882655|electronics.smart...| samsung| 900.64|530282093|50a293fb-5940-41b...|\n",
      "|2019-10-01 00:00:...|      view|   3900746|2053013552326770905|appliances.enviro...|   haier| 102.38|555444559|98b88fa0-d8fa-4b9...|\n",
      "|2019-10-01 00:00:...|      view|  44600062|2103807459595387724|                NULL|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|\n",
      "|2019-10-01 00:00:...|      view|  13500240|2053013557099889147|furniture.bedroom...|     brw|  93.18|555446365|7f0062d8-ead0-4e0...|\n",
      "|2019-10-01 00:00:...|      view|  23100006|2053013561638126333|                NULL|    NULL| 357.79|513642368|17566c27-0a8f-450...|\n",
      "|2019-10-01 00:00:...|      view|   1801995|2053013554415534427|electronics.video.tv|   haier| 193.03|537192226|e3151795-c355-4ef...|\n",
      "|2019-10-01 00:00:...|      view|  10900029|2053013555069845885|appliances.kitche...|   bosch|  58.95|519528062|901b9e3c-3f8f-414...|\n",
      "|2019-10-01 00:00:...|      view|   1306631|2053013558920217191|  computers.notebook|      hp| 580.89|550050854|7c90fc70-0e80-459...|\n",
      "|2019-10-01 00:00:...|      view|   1005135|2053013555631882655|electronics.smart...|   apple|1747.79|535871217|c6bd7419-2748-4c5...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "ecomm_oct_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:42:00.398013Z",
     "start_time": "2025-09-27T19:41:58.099569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "42448764"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecomm_oct_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:42:01.252814Z",
     "start_time": "2025-09-27T19:42:01.228202Z"
    }
   },
   "outputs": [],
   "source": [
    "ecomm_nov_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .schema(schema)\n",
    "    .load(november_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:42:01.926598Z",
     "start_time": "2025-09-27T19:42:01.861272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|   brand| price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "|2019-11-01 00:00:...|      view|   1003461|2053013555631882655|electronics.smart...|  xiaomi|489.07|520088904|4d3b30da-a5e4-49d...|\n",
      "|2019-11-01 00:00:...|      view|   5000088|2053013566100866035|appliances.sewing...|  janome|293.65|530496790|8e5f4f83-366c-4f7...|\n",
      "|2019-11-01 00:00:...|      view|  17302664|2053013553853497655|                NULL|   creed| 28.31|561587266|755422e7-9040-477...|\n",
      "|2019-11-01 00:00:...|      view|   3601530|2053013563810775923|appliances.kitche...|      lg|712.87|518085591|3bfb58cd-7892-48c...|\n",
      "|2019-11-01 00:00:...|      view|   1004775|2053013555631882655|electronics.smart...|  xiaomi|183.27|558856683|313628f1-68b8-460...|\n",
      "|2019-11-01 00:00:...|      view|   1306894|2053013558920217191|  computers.notebook|      hp|360.09|520772685|816a59f3-f5ae-4cc...|\n",
      "|2019-11-01 00:00:...|      view|   1306421|2053013558920217191|  computers.notebook|      hp|514.56|514028527|df8184cc-3694-454...|\n",
      "|2019-11-01 00:00:...|      view|  15900065|2053013558190408249|                NULL| rondell| 30.86|518574284|5e6ef132-4d7c-473...|\n",
      "|2019-11-01 00:00:...|      view|  12708937|2053013553559896355|                NULL|michelin| 72.72|532364121|0a899268-31eb-46d...|\n",
      "|2019-11-01 00:00:...|      view|   1004258|2053013555631882655|electronics.smart...|   apple|732.07|532647354|d2d3d2c6-631d-489...|\n",
      "|2019-11-01 00:00:...|      view|  17200570|2053013559792632471|furniture.living_...|    NULL|437.33|518780843|aa806835-b14c-45a...|\n",
      "|2019-11-01 00:00:...|      view|   2701517|2053013563911439225|appliances.kitche...|    NULL|155.11|518427361|c89b0d96-247f-404...|\n",
      "|2019-11-01 00:00:...|      view|  16700260|2053013559901684381|furniture.kitchen...|    NULL| 31.64|566255262|173d7b72-1db7-463...|\n",
      "|2019-11-01 00:00:...|      view|  34600011|2060981320581906480|                NULL|    NULL| 20.54|512416379|4dfe2c67-e537-4dc...|\n",
      "|2019-11-01 00:00:...|      view|   4600658|2053013563944993659|appliances.kitche...| samsung|411.83|526595547|aab33a9a-29c3-4d5...|\n",
      "|2019-11-01 00:00:...|      view|  24900193|2053013562183385881|                NULL|    NULL|  1.09|512651494|f603c815-f51a-46f...|\n",
      "|2019-11-01 00:00:...|      view|  27400066|2053013563391345499|                NULL|    NULL|  8.55|551061950|3f6112f1-5695-4e8...|\n",
      "|2019-11-01 00:00:...|      view|   5100503|2053013553375346967|                NULL|  xiaomi| 22.68|520037415|f54fa96a-f3f2-43a...|\n",
      "|2019-11-01 00:00:...|      view|   1004566|2053013555631882655|electronics.smart...|  huawei|164.84|566265908|52c2c76c-b79e-479...|\n",
      "|2019-11-01 00:00:...|      view|   1307115|2053013558920217191|  computers.notebook|      hp|411.59|514028527|df8184cc-3694-454...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "ecomm_nov_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:42:05.817767Z",
     "start_time": "2025-09-27T19:42:02.767435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "67501979"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecomm_nov_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create the Initial Parquet Tables \n",
    "> These will live outside of MinIO and the Iceberg Catalog\n",
    "\n",
    "* We'll create a simple helper function to _aid_ in our journey. We'll call it `write_parquet`.\n",
    "* Using the new _function_, we'll then need to convert the **csv**->**parquet**, so run the **two** cells under \"convert the data\". They will produce raw parquet records under the path `datasets/ecomm_raw/parquet/ecomm`.\n",
    "* We will then be able to run a series of 'daily' transactions to **write** all of the records into our foundational **iceberg** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:42:07.660137Z",
     "start_time": "2025-09-27T19:42:07.632118Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_parquet(df: DataFrame, destination: Path, sink_dir: str) -> DataFrame:\n",
    "    save_path = destination.joinpath('parquet', sink_dir)\n",
    "    # modifications to the dataframe\n",
    "    transformed = (\n",
    "        df\n",
    "            .withColumn(\"event_time\", to_timestamp(col(\"event_time\"), \"yyyy-MM-dd HH:mm:ss z\"))\n",
    "            .withColumn(\"event_date\", to_date(col(\"event_time\")))\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        transformed\n",
    "            .write\n",
    "            .format(\"parquet\")\n",
    "            .partitionBy(\"event_date\")\n",
    "            .mode(\"append\")\n",
    "            .save(save_path.as_posix())\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Convert the Data\n",
    "We're on a mission to convert CSV to Parquet. Let's do that next.\n",
    "> Note: This process may take a while if you're using the full dataset (~19GB)\n",
    "> Note: The function \"expects\" that we'll be _appending__ to the **ecomm** directory.\n",
    "> * If you want to modify the behavior of the function, give it a new argument called mode, and default that to \"errorIfExists\" of \"ignore\" - so you don't have to worry about deduplication or going nuculear and wiping out the entire ecomm directory!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:42:12.616416Z",
     "start_time": "2025-09-27T19:42:12.605303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh good. Saved you from having to import again. Not to mention the duplicates - what a mess\n"
     ]
    }
   ],
   "source": [
    "# note: ecomm_raw_dir is the Path instance to the datasets/ecomm_raw directory\n",
    "# further note: this could take a few minutes locally. Just turn up your favorite jams and let it ride\n",
    "# lastly, the safe guard is on to check if the parquet directory already exists. This is to save you from accidentally \n",
    "# running the import more than once. This is to save you the trouble this can cause.\n",
    "\n",
    "if not ecomm_raw_dir.joinpath('parquet', 'ecomm', 'event_date=2019-10-01').is_dir():\n",
    "  write_parquet(ecomm_oct_df, ecomm_raw_dir, 'ecomm')\n",
    "else:\n",
    "  print(\"oh good. Saved you from having to import again. Not to mention the duplicates - what a mess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:42:16.050048Z",
     "start_time": "2025-09-27T19:42:16.044031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same goes for the november set. It exists, so we'll skip for now.\n"
     ]
    }
   ],
   "source": [
    "if not ecomm_raw_dir.joinpath('parquet', 'ecomm', 'event_date=2019-11-01').is_dir():\n",
    "  write_parquet(ecomm_nov_df, ecomm_raw_dir, 'ecomm')\n",
    "else:\n",
    "  print(\"same goes for the november set. It exists, so we'll skip for now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Iceberg: Creating and Appending data to Iceberg Tables\n",
    "> This section will help us build a firm understanding of how to work with Iceberg Tables\n",
    "1. Learn to easily check for table existence\n",
    "2. Learn to use Spark SQL to create Namespaces (this is where our tables live)\n",
    "3. Use the simple table existence check to either a) create a new Iceberg table, or b) append new rows to an existing table within a Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:42:18.339703Z",
     "start_time": "2025-09-27T19:42:18.307566Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's identify a helper function to check to see if a \"table\" exists\n",
    "# this can be done a few different ways, but this one uses the 'underlying' spark.catalog\n",
    "# and I like that better :)\n",
    "\n",
    "def table_exists(table_name: str):\n",
    "    return any(table.name == table_name for table in spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tables live in Namespaces\n",
    "Namespaces are a way of **grouping** tables together in the same way you would use **data domains** to separate different categories of data. \n",
    "> In practice, a namespace could be something like \"consumer\", \"product\", \"orders\" for ecommerce.\n",
    "> For our demo, we'll just call the namespace `icystreams` since we are grouping data to use for _Streaming Iceberg_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:42:23.866275Z",
     "start_time": "2025-09-27T19:42:23.732152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| namespace|\n",
      "+----------+\n",
      "|icystreams|\n",
      "+----------+\n"
     ]
    }
   ],
   "source": [
    "catalog_namespace = 'icystreams'\n",
    "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {catalog_namespace}\")\n",
    "spark.sql(\"SHOW NAMESPACES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:42:32.514611Z",
     "start_time": "2025-09-27T19:42:32.486475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'lakekeeper'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The current catalog will be the value of the following configuration: spark.conf.get(\"spark.sql.defaultCatalog\")\n",
    "spark.catalog.currentCatalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:42:33.316550Z",
     "start_time": "2025-09-27T19:42:33.173044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Database(name='icystreams', catalog='lakekeeper', description=None, locationUri='s3://examples/initial-warehouse/01998cb2-ac2e-7cb0-b341-ae5794445827')]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# within the Apache Spark ecosystem. Our namespace is equivalent to a traditional hive \"database\", or Unity Catalog \"schema\"\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> Note: To simplify how we write to Iceberg, we are going to use the `spark.catalog.*` functions to point to `lakekeeper.icystreams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:42:34.964545Z",
     "start_time": "2025-09-27T19:42:34.953419Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase('icystreams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:42:57.867247Z",
     "start_time": "2025-09-27T19:42:57.860574Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_iceberg(df: DataFrame, namespace: str, table_name: str, partition_col: str) -> DataFrame:\n",
    "    # simplifies the process of `testing` for the state of a given Iceberg table\n",
    "    to_iceberg = (\n",
    "        df\n",
    "            .writeTo(f\"{namespace}.{table_name}\")\n",
    "            .partitionedBy(partition_col)\n",
    "    )\n",
    "    # decided to append or create\n",
    "    if table_exists(table_name):\n",
    "        return to_iceberg.append()\n",
    "    return to_iceberg.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:15:12.998260Z",
     "start_time": "2025-09-27T20:15:12.956297Z"
    }
   },
   "outputs": [],
   "source": [
    "# provide a way of creating hidden partitions\n",
    "# note: check out all the tableproperties here (https://iceberg.apache.org/docs/latest/configuration/)\n",
    "def write_iceberg_hidden_partitions(df: DataFrame, namespace: str, table_name: str, partitioned_by_cols: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    This will create a new iceberg table (if it doesn't exist)\n",
    "    :param df: The reference DataFrame to steal our schema from\n",
    "    :param namespace: The namespace to write the table within\n",
    "    :param table_name: The name of the table\n",
    "    :param partitioned_by_cols: The PARTITIONED BY (clause). Example: 'days(event_time), category_id'\n",
    "    :return: an empty DataFrame on success\n",
    "    \"\"\"\n",
    "    # todo - add default ordering : ORDERED BY (category ASC, event_time DESC)\n",
    "    if not table_exists(table_name):\n",
    "        # fetch the ddl from the dataframe schema\n",
    "        table_ddl = df.schema.toDDL()\n",
    "        res_df = spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {namespace}.{table_name} (\n",
    "                {table_ddl}\n",
    "            ) USING ICEBERG\n",
    "            PARTITIONED BY ({partitioned_by_cols})\n",
    "            TBLPROPERTIES(\n",
    "            'read.split.planning-lookback'='5',\n",
    "            'read.parquet.vectorization.batch-size'='32'\n",
    "            );\n",
    "        \"\"\")\n",
    "        print(res_df.show())\n",
    "    \n",
    "    # since we explicitly create the table, we can simplify this vs the other version of the function\n",
    "    return df.writeTo(f\"{namespace}.{table_name}\").append()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Writing our Parquet into Iceberg\n",
    "The following two cells will copy all data from the October and November 2019 parquet records\n",
    "into the Iceberg tables `ecomm` and `ecomm_hidden`. \n",
    "\n",
    "The big difference here is that we don't need to \"add\" additional columns to the source parquet to this functionality.\n",
    "> Note: If you are familiar with generated columns (generateAlwaysAs), then think of the PARTITIONED BY(days(event_time), bucket(8, category_id)) \n",
    "> as the same kind of mechanic. Remember, you can always change your mind later and drop or add additional partitoning without\n",
    "> rewriting the entire table. Enjoy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# set the reference to the parquet directory\n",
    "parquet_dir = ecomm_raw_dir.joinpath('parquet', 'ecomm')\n",
    "iceberg_table_name = 'ecomm'\n",
    "iceberg_table_name_hp = \"ecomm_hidden\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:16:06.004139Z",
     "start_time": "2025-09-27T20:16:05.989895Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T19:59:01.982687Z",
     "start_time": "2025-09-27T19:56:55.767438Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run the following block to write a single transaction per day\n",
    "# Note: This took around 2 minutes on a high-powered M3 Ultra computer - 10 core (eg: spark.master=local[*])\n",
    "# This can run on a single machine really easily - but blindingly fast in a cluster (only latency is IO between your object storage and your Iceberg Rest Catalog API and the Spark Cluster)\n",
    "year = \"2019\"\n",
    "months = ['10', '11']\n",
    "for month in list(months):\n",
    "    for day in (range(1, 32)) if month == '10' else (range(1, 31)):\n",
    "        insert_date = f\"{year}-{month}-0{day}\" if day < 10 else f\"{year}-{month}-{day}\"\n",
    "        write_iceberg(\n",
    "            (spark\n",
    "                .read\n",
    "                .format('parquet')\n",
    "                .load(parquet_dir.as_posix())\n",
    "                .where(col(\"event_date\").isin(insert_date))\n",
    "            ),\n",
    "        catalog_namespace,\n",
    "        iceberg_table_name,\n",
    "        'event_date'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    total|\n",
      "+---------+\n",
      "|109950743|\n",
      "+---------+\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select count(*) as total from {catalog_namespace}.{iceberg_table_name}\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:01:15.954295Z",
     "start_time": "2025-09-27T20:01:15.833143Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------------------+-------------------+\n",
      "|product_id|total|           min_time|           max_time|\n",
      "+----------+-----+-------------------+-------------------+\n",
      "|  17302664| 2573|2019-10-01 18:00:17|2019-11-12 15:48:40|\n",
      "|   1003461| 3911|2019-10-01 18:07:04|2019-11-12 15:56:11|\n",
      "+----------+-----+-------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# shows a query that can do mix/max pushdown with simple partition by on 'event_date'\n",
    "spark.sql(f\"\"\"\n",
    "select\n",
    "  product_id, \n",
    "  count(*) as total,\n",
    "  min(event_time) as min_time, \n",
    "  max(event_time) as max_time \n",
    "  from {catalog_namespace}.{iceberg_table_name}\n",
    "  where product_id in (17302664, 1003461) \n",
    "  and event_time BETWEEN TIMESTAMP('2019-10-01 18:00:00') and TIMESTAMP('2019-11-12 16:00:00') \n",
    "  group by product_id\n",
    "\"\"\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:04:16.585614Z",
     "start_time": "2025-09-27T20:04:15.741951Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# writing all the same data just with hidden partitioning\n",
    "# note: there are only 691 categories, while we have over 200k unique product identifiers\n",
    "# bucketing by product_id is another strategy to reduce the directory cost with hidden partitions\n",
    "# try some things, break some things, it is fine :)\n",
    "use_ddl_table = \"ecomm_hidden_other\"\n",
    "year = \"2019\"\n",
    "months = ['10', '11']\n",
    "for month in list(months):\n",
    "    for day in (range(1, 32)) if month == '10' else (range(1, 31)):\n",
    "        insert_date = f\"{year}-{month}-0{day}\" if day < 10 else f\"{year}-{month}-{day}\"\n",
    "        write_iceberg_hidden_partitions(\n",
    "            df=(\n",
    "                spark.read\n",
    "                .format('parquet')\n",
    "                .load(parquet_dir.as_posix())\n",
    "                .where(col(\"event_date\").isin(insert_date))\n",
    "                .drop(col(\"event_date\"))\n",
    "            ),\n",
    "            namespace=catalog_namespace,\n",
    "            table_name=use_ddl_table,\n",
    "            partitioned_by_cols='days(event_time), bucket(8, category_id)'\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+------------+-------+---------+--------------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|       brand|  price|  user_id|        user_session|\n",
      "+-------------------+----------+----------+-------------------+--------------------+------------+-------+---------+--------------------+\n",
      "|2019-10-20 12:51:37|      view|  15200217|2053013553484398879|                NULL|        NULL| 123.56|545081263|c2e016df-d267-4cf...|\n",
      "|2019-10-20 12:51:38|      view|   1801881|2053013554415534427|electronics.video.tv|     samsung| 493.93|514802508|ab915d15-8aae-49e...|\n",
      "|2019-10-20 12:51:38|      view|   5801039|2053013553945772349|electronics.audio...|       hertz| 132.64|542448535|98dc10fa-427f-45f...|\n",
      "|2019-10-20 12:51:38|  purchase|   5100742|2053013553341792533|  electronics.clocks|       honor| 148.01|517816925|6cba4ff0-0b07-4d8...|\n",
      "|2019-10-20 12:51:39|      view|   1802064|2053013554415534427|electronics.video.tv|     philips| 566.27|551443583|eba53a23-34ec-456...|\n",
      "|2019-10-20 12:51:39|      view|   1801830|2053013554415534427|electronics.video.tv|       yasin| 158.05|562241117|773b9116-15bd-41a...|\n",
      "|2019-10-20 12:51:39|      view|   1801696|2053013554415534427|electronics.video.tv|     hisense| 772.19|514563875|24fd6e74-8cb7-47c...|\n",
      "|2019-10-20 12:51:39|      view|  23200391|2053013562359546659|                NULL|coopervision|  21.88|513368418|2de35e84-469c-479...|\n",
      "|2019-10-20 12:51:40|      view|   7600132|2053013552821698803|                NULL|     tp-link|  20.05|512964081|f213b8e7-5869-47f...|\n",
      "|2019-10-20 12:51:40|      view|   1801691|2053013554415534427|electronics.video.tv|     samsung| 475.82|562246044|386818ac-050d-43e...|\n",
      "|2019-10-20 12:51:40|      view|   5100338|2053013553341792533|  electronics.clocks|       apple| 334.11|552393607|826ae7c8-4df1-44f...|\n",
      "|2019-10-20 12:51:41|      view|   1801513|2053013554415534427|electronics.video.tv|       haier| 489.05|525015316|1a4c7384-7550-469...|\n",
      "|2019-10-20 12:51:41|      view|   1801929|2053013554415534427|electronics.video.tv|     samsung| 559.96|515370141|fc142478-427b-4ab...|\n",
      "|2019-10-20 12:51:41|      view|   5301045|2053013563173241677|                NULL|      galaxy|   9.24|518645141|2b270c0e-5df9-4de...|\n",
      "|2019-10-20 12:51:41|      view|  15200281|2053013553484398879|                NULL|        akom|  64.35|555016901|d58d4024-55d3-41f...|\n",
      "|2019-10-20 12:51:42|      view|   1801854|2053013554415534427|electronics.video.tv|     samsung|1003.58|540466469|498e5623-8778-451...|\n",
      "|2019-10-20 12:51:42|      view|   5801483|2053013553945772349|electronics.audio...|     pioneer|  58.95|562245399|b74ccaff-8afc-46d...|\n",
      "|2019-10-20 12:51:42|      view|   1701505|2053013553031414015|computers.periphe...|     huntkey| 172.72|525684135|bb580390-1926-439...|\n",
      "|2019-10-20 12:51:43|      view|  46300025|2110937218225800069|       apparel.dress|     aivengo|  34.22|514445420|37b12322-0f3e-411...|\n",
      "|2019-10-20 12:51:43|      view|   2501143|2053013564003713919|appliances.kitche...|       artel|  36.01|535893988|f1ed7c2f-7789-4cb...|\n",
      "+-------------------+----------+----------+-------------------+--------------------+------------+-------+---------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "hidden_parts_df = spark.sql(f\"select * from {catalog_namespace}.{use_ddl_table}\")\n",
    "hidden_parts_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:19:22.966622Z",
     "start_time": "2025-09-27T20:19:22.783781Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------------------+-------------------+\n",
      "|product_id|total|           min_time|           max_time|\n",
      "+----------+-----+-------------------+-------------------+\n",
      "|  49500003| 2054|2019-10-02 17:08:09|2019-11-01 18:21:43|\n",
      "|  13200834| 9778|2019-10-01 18:21:21|2019-11-12 15:59:27|\n",
      "|  49300028|  217|2019-10-02 00:51:05|2019-10-28 16:02:04|\n",
      "|  17800204| 3519|2019-10-01 18:54:18|2019-11-12 15:57:24|\n",
      "|   2700658|  452|2019-10-02 09:12:59|2019-11-12 15:08:43|\n",
      "|  11500291| 1183|2019-10-01 18:19:03|2019-11-12 15:57:30|\n",
      "|  13201247|  277|2019-10-02 06:06:14|2019-11-12 10:49:36|\n",
      "|   2702332| 3362|2019-10-01 18:19:38|2019-11-12 15:58:09|\n",
      "|   9100019|  285|2019-10-01 20:06:03|2019-11-12 15:33:22|\n",
      "|   9101528|  174|2019-10-02 06:22:15|2019-11-12 05:55:10|\n",
      "|  35108841|   13|2019-10-07 12:57:28|2019-11-08 14:50:27|\n",
      "|  11500489|  170|2019-10-01 18:11:57|2019-11-12 06:34:15|\n",
      "|   1600437|  228|2019-10-02 03:38:48|2019-11-12 08:23:19|\n",
      "|   6100213|  208|2019-10-02 10:42:16|2019-11-12 14:49:28|\n",
      "|  13200148|  352|2019-10-02 08:34:35|2019-11-12 15:21:48|\n",
      "|  34800380|   92|2019-10-08 17:02:34|2019-11-12 15:59:59|\n",
      "|   2700779|  585|2019-10-02 03:30:34|2019-11-12 10:07:45|\n",
      "|  19700152|   40|2019-10-05 17:08:31|2019-11-12 13:29:39|\n",
      "|   1600635|  159|2019-10-08 03:56:04|2019-11-12 12:42:28|\n",
      "|  17800052|  245|2019-10-01 18:22:10|2019-11-12 15:45:25|\n",
      "+----------+-----+-------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "select\n",
    "  product_id, \n",
    "  count(*) as total,\n",
    "  min(event_time) as min_time, \n",
    "  max(event_time) as max_time \n",
    "  from {catalog_namespace}.{use_ddl_table}\n",
    "  where product_id not in (17302664, 1003461)\n",
    "  and event_time BETWEEN TIMESTAMP('2019-10-01 18:00:00') and TIMESTAMP('2019-11-12 16:00:00') \n",
    "  group by product_id\n",
    "\"\"\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:20:00.795273Z",
     "start_time": "2025-09-27T20:19:57.389293Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " createtab_stmt | CREATE TABLE lakekeeper.icystreams.ecomm_hidden_other (\\n  event_time TIMESTAMP,\\n  event_type STRING,\\n  product_id INT,\\n  category_id BIGINT,\\n  category_code STRING,\\n  brand STRING,\\n  price FLOAT,\\n  user_id INT,\\n  user_session STRING)\\nUSING iceberg\\nPARTITIONED BY (days(event_time))\\nCLUSTERED BY (category_id)\\nINTO 8 BUCKETS\\nLOCATION 's3://examples/initial-warehouse/01998cb2-ac2e-7cb0-b341-ae5794445827/01998cd3-0af8-7be3-a93a-28f07bb9d4ca'\\nTBLPROPERTIES (\\n  'current-snapshot-id' = '6151459368212957826',\\n  'format' = 'iceberg/parquet',\\n  'format-version' = '2',\\n  'read.parquet.vectorization.batch-size' = '32',\\n  'read.split.planning-lookback' = '5')\\n \n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"show create table {catalog_namespace}.{use_ddl_table}\").show(100, 0, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:22:01.150677Z",
     "start_time": "2025-09-27T20:22:01.079197Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    total|\n",
      "+---------+\n",
      "|109950743|\n",
      "+---------+\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select count(*) as total from {catalog_namespace}.{iceberg_table_name_hp}\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:01:11.079115Z",
     "start_time": "2025-09-27T20:01:10.960085Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "             category_id\n0    2060237588744111062\n1    2090971686529663114\n2    2053013564003713919\n3    2053013556344914381\n4    2152167773222993940\n..                   ...\n686  2100064855133258156\n687  2053013560111399597\n688  2053013552259662037\n689  2055156924273394455\n690  2187707857414128194\n\n[691 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2060237588744111062</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2090971686529663114</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2053013564003713919</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2053013556344914381</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2152167773222993940</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>686</th>\n      <td>2100064855133258156</td>\n    </tr>\n    <tr>\n      <th>687</th>\n      <td>2053013560111399597</td>\n    </tr>\n    <tr>\n      <th>688</th>\n      <td>2053013552259662037</td>\n    </tr>\n    <tr>\n      <th>689</th>\n      <td>2055156924273394455</td>\n    </tr>\n    <tr>\n      <th>690</th>\n      <td>2187707857414128194</td>\n    </tr>\n  </tbody>\n</table>\n<p>691 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"select distinct(category_id) as category_id from {catalog_namespace}.{iceberg_table_name_hp}\").toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:11:44.236345Z",
     "start_time": "2025-09-27T20:11:40.893449Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|unique_product_ids|\n",
      "+------------------+\n",
      "|            206876|\n",
      "+------------------+\n"
     ]
    }
   ],
   "source": [
    "# Note: we've got 206,876 unique product ids. This would be a bad column to partition on\n",
    "spark.sql(f\"select count(distinct(product_id)) as unique_product_ids from {catalog_namespace}.{iceberg_table_name_hp}\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:08:13.969322Z",
     "start_time": "2025-09-27T20:08:10.396858Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|unique_category_ids|\n",
      "+-------------------+\n",
      "|                691|\n",
      "+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# Note: the cardinality of the category id's is only 691 distinct\n",
    "spark.sql(f\"select count(distinct(category_id)) as unique_category_ids from {catalog_namespace}.{iceberg_table_name_hp}\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:08:39.255691Z",
     "start_time": "2025-09-27T20:08:36.124342Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "        product_id\n0         28300055\n1          1307545\n2         38900025\n3          5500325\n4          4900373\n...            ...\n206871   100025269\n206872    26006635\n206873    24300264\n206874    10800003\n206875    26600968\n\n[206876 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>28300055</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1307545</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38900025</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5500325</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4900373</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>206871</th>\n      <td>100025269</td>\n    </tr>\n    <tr>\n      <th>206872</th>\n      <td>26006635</td>\n    </tr>\n    <tr>\n      <th>206873</th>\n      <td>24300264</td>\n    </tr>\n    <tr>\n      <th>206874</th>\n      <td>10800003</td>\n    </tr>\n    <tr>\n      <th>206875</th>\n      <td>26600968</td>\n    </tr>\n  </tbody>\n</table>\n<p>206876 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"select distinct(product_id) as product_id from {catalog_namespace}.{iceberg_table_name_hp}\").toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:12:24.410896Z",
     "start_time": "2025-09-27T20:12:20.504896Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------------------+-------------------+\n",
      "|product_id|total|           min_time|           max_time|\n",
      "+----------+-----+-------------------+-------------------+\n",
      "|  28715821|  777|2019-10-02 06:59:28|2019-11-12 06:09:49|\n",
      "|  28706430| 2589|2019-10-02 00:35:31|2019-11-03 15:06:39|\n",
      "|  15600022| 1089|2019-10-01 18:18:07|2019-11-12 14:02:58|\n",
      "|  26600154|  148|2019-10-02 12:51:58|2019-11-12 15:36:18|\n",
      "|   4300425|  109|2019-10-02 06:36:54|2019-11-12 13:07:04|\n",
      "|  28703068|   76|2019-10-12 01:43:51|2019-11-12 15:30:56|\n",
      "|  47000059|  455|2019-10-04 05:54:34|2019-11-12 15:19:28|\n",
      "|  30000077|  758|2019-10-01 23:12:23|2019-11-12 14:48:08|\n",
      "|  22200112|  173|2019-10-01 19:52:42|2019-11-12 12:11:30|\n",
      "|  52900142|  182|2019-10-10 02:51:48|2019-11-12 15:28:07|\n",
      "|  45600191|  267|2019-10-16 11:24:10|2019-11-10 15:19:18|\n",
      "|   4300232|  158|2019-10-09 13:53:21|2019-11-01 05:46:16|\n",
      "|   1480506| 1477|2019-10-01 18:10:39|2019-11-12 03:55:18|\n",
      "|  17100091|   89|2019-10-01 18:08:59|2019-11-12 11:27:18|\n",
      "|   1480743| 1258|2019-10-03 05:18:59|2019-11-12 15:50:35|\n",
      "|  28721421|  143|2019-10-18 06:29:50|2019-11-12 15:09:24|\n",
      "|  15900439|   42|2019-10-05 05:56:57|2019-11-05 02:28:27|\n",
      "|  39800101|   15|2019-10-03 20:35:43|2019-11-04 10:53:22|\n",
      "|  43900084|   91|2019-10-07 09:20:41|2019-11-12 07:14:43|\n",
      "|  26600131|  185|2019-10-03 15:50:55|2019-11-12 15:41:23|\n",
      "+----------+-----+-------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# shows a query that can do mix/max pushdown and leans into the hidden date partitions to do file skipping\n",
    "# you'll notice that we can do arbitrary filtering by hour as well.\n",
    "# this query isn't taking advantage of category filtering (see how the two tables compare as homework)\n",
    "spark.sql(f\"\"\"\n",
    "select\n",
    "  product_id, \n",
    "  count(*) as total,\n",
    "  min(event_time) as min_time, \n",
    "  max(event_time) as max_time \n",
    "  from {catalog_namespace}.{iceberg_table_name_hp}\n",
    "  where product_id not in (17302664, 1003461)\n",
    "  and event_time BETWEEN TIMESTAMP('2019-10-01 18:00:00') and TIMESTAMP('2019-11-12 16:00:00') \n",
    "  group by product_id\n",
    "\"\"\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:27:40.450714Z",
     "start_time": "2025-09-27T20:27:37.121169Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:13:57.315254Z",
     "start_time": "2025-09-27T20:13:57.284541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                      createtab_stmt\n0  CREATE TABLE lakekeeper.icystreams.ecomm (\\n  ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>createtab_stmt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CREATE TABLE lakekeeper.icystreams.ecomm (\\n  ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the create table statement\n",
    "spark.sql(f\"show create table {catalog_namespace}.{iceberg_table_name}\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:13:37.656740Z",
     "start_time": "2025-09-27T20:13:37.601122Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                        col_name  \\\n0                     event_time   \n1                     event_type   \n2                     product_id   \n3                    category_id   \n4                  category_code   \n5                          brand   \n6                          price   \n7                        user_id   \n8                   user_session   \n9                     event_date   \n10       # Partition Information   \n11                    # col_name   \n12                    event_date   \n13                                 \n14            # Metadata Columns   \n15                      _spec_id   \n16                    _partition   \n17                         _file   \n18                          _pos   \n19                      _deleted   \n20                                 \n21  # Detailed Table Information   \n22                          Name   \n23                          Type   \n24                      Location   \n25                      Provider   \n26                         Owner   \n27              Table Properties   \n28                    Statistics   \n\n                                            data_type  comment  \n0                                           timestamp     None  \n1                                              string     None  \n2                                                 int     None  \n3                                              bigint     None  \n4                                              string     None  \n5                                              string     None  \n6                                               float     None  \n7                                                 int     None  \n8                                              string     None  \n9                                                date     None  \n10                                                              \n11                                          data_type  comment  \n12                                               date     None  \n13                                                              \n14                                                              \n15                                                int           \n16                            struct<event_date:date>           \n17                                             string           \n18                                             bigint           \n19                                            boolean           \n20                                                              \n21                                                              \n22                        lakekeeper.icystreams.ecomm           \n23                                            MANAGED           \n24  s3://examples/initial-warehouse/01998cb2-ac2e-...           \n25                                            iceberg           \n26                                             jovyan           \n27  [current-snapshot-id=3591208576167435970,forma...           \n28                  12314483216 bytes, 109950743 rows     None  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>col_name</th>\n      <th>data_type</th>\n      <th>comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>event_time</td>\n      <td>timestamp</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>event_type</td>\n      <td>string</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>product_id</td>\n      <td>int</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>category_id</td>\n      <td>bigint</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>category_code</td>\n      <td>string</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>brand</td>\n      <td>string</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>price</td>\n      <td>float</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>user_id</td>\n      <td>int</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>user_session</td>\n      <td>string</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>event_date</td>\n      <td>date</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td># Partition Information</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td># col_name</td>\n      <td>data_type</td>\n      <td>comment</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>event_date</td>\n      <td>date</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td># Metadata Columns</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>_spec_id</td>\n      <td>int</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>_partition</td>\n      <td>struct&lt;event_date:date&gt;</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>_file</td>\n      <td>string</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>_pos</td>\n      <td>bigint</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>_deleted</td>\n      <td>boolean</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td># Detailed Table Information</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Name</td>\n      <td>lakekeeper.icystreams.ecomm</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Type</td>\n      <td>MANAGED</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Location</td>\n      <td>s3://examples/initial-warehouse/01998cb2-ac2e-...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Provider</td>\n      <td>iceberg</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Owner</td>\n      <td>jovyan</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Table Properties</td>\n      <td>[current-snapshot-id=3591208576167435970,forma...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Statistics</td>\n      <td>12314483216 bytes, 109950743 rows</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"describe extended {catalog_namespace}.{iceberg_table_name}\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T00:57:55.189719Z",
     "start_time": "2025-09-27T00:57:54.349194Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109950743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total\n",
       "0  109950743"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"select count(*) as total from {catalog_namespace}.{iceberg_table_name}\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T20:20:32.423543Z",
     "start_time": "2025-09-27T20:20:30.347787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            event_time event_type  product_id          category_id  \\\n0  2019-11-30 00:00:00       view     4804055  2053013554658804075   \n1  2019-11-30 00:00:00       view     1004225  2053013555631882655   \n2  2019-11-30 00:00:00       view    12703410  2053013553559896355   \n3  2019-11-30 00:00:01       view     3601438  2053013563810775923   \n4  2019-11-30 00:00:02       view     6400179  2053013554121933129   \n5  2019-11-30 00:00:02       view    18400149  2053013553912217915   \n6  2019-11-30 00:00:02       view    40900011  2127425434894205468   \n7  2019-11-30 00:00:02       view     3500093  2053013555287949705   \n8  2019-11-30 00:00:03       view     6000167  2053013560807654091   \n9  2019-11-30 00:00:03       view     1004225  2053013555631882655   \n10 2019-11-30 00:00:03       view     2900958  2053013554776244595   \n11 2019-11-30 00:00:03       view    12705001  2053013553559896355   \n12 2019-11-30 00:00:04       view    15201321  2053013553484398879   \n13 2019-11-30 00:00:04       view     1305999  2053013558920217191   \n14 2019-11-30 00:00:04       view     6700796  2053013554247762257   \n\n                      category_code      brand        price    user_id  \\\n0       electronics.audio.headphone      apple   196.270004  548106089   \n1            electronics.smartphone      apple   952.150024  579329479   \n2                              None   cordiant    47.619999  574179352   \n3         appliances.kitchen.washer       beko   215.889999  555019938   \n4          computers.components.cpu      intel   203.869995  513342746   \n5                              None       kicx   112.489998  521867315   \n6       construction.tools.painting       None    48.910000  569407672   \n7                              None       None    33.459999  553748453   \n8            auto.accessories.alarm  centurion    82.110001  547094018   \n9            electronics.smartphone      apple   952.150024  579329479   \n10     appliances.kitchen.microwave        arg    59.180000  574140756   \n11                             None   cordiant    41.959999  547095022   \n12                             None      bosch   120.699997  542501188   \n13               computers.notebook      apple  2273.419922  568658074   \n14  computers.components.videocards   gigabyte   165.229996  564208988   \n\n                            user_session  event_date  \n0   e0b70673-d26d-4c72-98e8-7c25bb8d1216  2019-11-30  \n1   e302bdd1-1f8c-4210-9920-a55036d2e8c9  2019-11-30  \n2   69e96ac7-8042-42bf-b145-40bc937dd141  2019-11-30  \n3   626534f8-eb2a-4d05-84d4-9b9d91edb9d8  2019-11-30  \n4   4ba0eec3-1026-4df4-9495-17773a86ead5  2019-11-30  \n5   824aa67e-7220-4735-9507-0f37b3d5c27b  2019-11-30  \n6   5d9b3bbd-4efc-4b7c-86dd-8cb9e2bc8f77  2019-11-30  \n7   4310c6c8-e10c-49db-a85c-051aaf0969d3  2019-11-30  \n8   cea57a4d-5343-4de2-81fc-48124020b8bf  2019-11-30  \n9   e302bdd1-1f8c-4210-9920-a55036d2e8c9  2019-11-30  \n10  a26912f8-2ee2-4579-99b1-5483bd6bb43f  2019-11-30  \n11  29e5279b-23a8-42a3-b337-e9110e4789e6  2019-11-30  \n12  78a1a976-3795-4f12-81e7-aaec699fab90  2019-11-30  \n13  4c04c422-dca3-48f0-8ae9-64653b9b4840  2019-11-30  \n14  2662d597-acb8-4adc-8951-923b1329362b  2019-11-30  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_time</th>\n      <th>event_type</th>\n      <th>product_id</th>\n      <th>category_id</th>\n      <th>category_code</th>\n      <th>brand</th>\n      <th>price</th>\n      <th>user_id</th>\n      <th>user_session</th>\n      <th>event_date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-11-30 00:00:00</td>\n      <td>view</td>\n      <td>4804055</td>\n      <td>2053013554658804075</td>\n      <td>electronics.audio.headphone</td>\n      <td>apple</td>\n      <td>196.270004</td>\n      <td>548106089</td>\n      <td>e0b70673-d26d-4c72-98e8-7c25bb8d1216</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-11-30 00:00:00</td>\n      <td>view</td>\n      <td>1004225</td>\n      <td>2053013555631882655</td>\n      <td>electronics.smartphone</td>\n      <td>apple</td>\n      <td>952.150024</td>\n      <td>579329479</td>\n      <td>e302bdd1-1f8c-4210-9920-a55036d2e8c9</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-11-30 00:00:00</td>\n      <td>view</td>\n      <td>12703410</td>\n      <td>2053013553559896355</td>\n      <td>None</td>\n      <td>cordiant</td>\n      <td>47.619999</td>\n      <td>574179352</td>\n      <td>69e96ac7-8042-42bf-b145-40bc937dd141</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-11-30 00:00:01</td>\n      <td>view</td>\n      <td>3601438</td>\n      <td>2053013563810775923</td>\n      <td>appliances.kitchen.washer</td>\n      <td>beko</td>\n      <td>215.889999</td>\n      <td>555019938</td>\n      <td>626534f8-eb2a-4d05-84d4-9b9d91edb9d8</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-11-30 00:00:02</td>\n      <td>view</td>\n      <td>6400179</td>\n      <td>2053013554121933129</td>\n      <td>computers.components.cpu</td>\n      <td>intel</td>\n      <td>203.869995</td>\n      <td>513342746</td>\n      <td>4ba0eec3-1026-4df4-9495-17773a86ead5</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2019-11-30 00:00:02</td>\n      <td>view</td>\n      <td>18400149</td>\n      <td>2053013553912217915</td>\n      <td>None</td>\n      <td>kicx</td>\n      <td>112.489998</td>\n      <td>521867315</td>\n      <td>824aa67e-7220-4735-9507-0f37b3d5c27b</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2019-11-30 00:00:02</td>\n      <td>view</td>\n      <td>40900011</td>\n      <td>2127425434894205468</td>\n      <td>construction.tools.painting</td>\n      <td>None</td>\n      <td>48.910000</td>\n      <td>569407672</td>\n      <td>5d9b3bbd-4efc-4b7c-86dd-8cb9e2bc8f77</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2019-11-30 00:00:02</td>\n      <td>view</td>\n      <td>3500093</td>\n      <td>2053013555287949705</td>\n      <td>None</td>\n      <td>None</td>\n      <td>33.459999</td>\n      <td>553748453</td>\n      <td>4310c6c8-e10c-49db-a85c-051aaf0969d3</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2019-11-30 00:00:03</td>\n      <td>view</td>\n      <td>6000167</td>\n      <td>2053013560807654091</td>\n      <td>auto.accessories.alarm</td>\n      <td>centurion</td>\n      <td>82.110001</td>\n      <td>547094018</td>\n      <td>cea57a4d-5343-4de2-81fc-48124020b8bf</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2019-11-30 00:00:03</td>\n      <td>view</td>\n      <td>1004225</td>\n      <td>2053013555631882655</td>\n      <td>electronics.smartphone</td>\n      <td>apple</td>\n      <td>952.150024</td>\n      <td>579329479</td>\n      <td>e302bdd1-1f8c-4210-9920-a55036d2e8c9</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2019-11-30 00:00:03</td>\n      <td>view</td>\n      <td>2900958</td>\n      <td>2053013554776244595</td>\n      <td>appliances.kitchen.microwave</td>\n      <td>arg</td>\n      <td>59.180000</td>\n      <td>574140756</td>\n      <td>a26912f8-2ee2-4579-99b1-5483bd6bb43f</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2019-11-30 00:00:03</td>\n      <td>view</td>\n      <td>12705001</td>\n      <td>2053013553559896355</td>\n      <td>None</td>\n      <td>cordiant</td>\n      <td>41.959999</td>\n      <td>547095022</td>\n      <td>29e5279b-23a8-42a3-b337-e9110e4789e6</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2019-11-30 00:00:04</td>\n      <td>view</td>\n      <td>15201321</td>\n      <td>2053013553484398879</td>\n      <td>None</td>\n      <td>bosch</td>\n      <td>120.699997</td>\n      <td>542501188</td>\n      <td>78a1a976-3795-4f12-81e7-aaec699fab90</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2019-11-30 00:00:04</td>\n      <td>view</td>\n      <td>1305999</td>\n      <td>2053013558920217191</td>\n      <td>computers.notebook</td>\n      <td>apple</td>\n      <td>2273.419922</td>\n      <td>568658074</td>\n      <td>4c04c422-dca3-48f0-8ae9-64653b9b4840</td>\n      <td>2019-11-30</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2019-11-30 00:00:04</td>\n      <td>view</td>\n      <td>6700796</td>\n      <td>2053013554247762257</td>\n      <td>computers.components.videocards</td>\n      <td>gigabyte</td>\n      <td>165.229996</td>\n      <td>564208988</td>\n      <td>2662d597-acb8-4adc-8951-923b1329362b</td>\n      <td>2019-11-30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "select * from {catalog_namespace}.{iceberg_table_name} \n",
    "where event_date BETWEEN DATE(\"2019-11-15\") AND DATE(\"2019-11-30\")\n",
    "ORDER BY event_date DESC\n",
    "LIMIT 15\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Is the Query Slow or is it Just your Laptop?\n",
    "These queries are super fast. But we're also not streaming yet. \n",
    "* We haven't optimized any of the tables at this point.\n",
    "* The next few cells show how to use the Iceberg stored procedures.  \n",
    "* Whether this is a local experiment or production, you'll still need to periodically \"clean\" your tables up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:24:05.822684Z",
     "start_time": "2025-09-27T20:24:04.015698Z"
    }
   },
   "outputs": [],
   "source": [
    "#result_df = spark.sql(f\"\"\"\n",
    "#CALL system.rewrite_data_files(\n",
    "#    table => 'lakekeeper.{catalog_namespace}.{iceberg_table_name}',\n",
    "#        options => map(\n",
    "#            'target-file-size-bytes', '134217728', -- 128MB target size\n",
    "#            'min-input-files', '5' -- Minimum files to trigger rewrite\n",
    "#        )\n",
    "#    );\n",
    "#\"\"\")\n",
    "\n",
    "# result_df = spark.sql(f\"CALL system.rewrite_data_files(table => 'lakekeeper.{catalog_namespace}.{iceberg_table_name}')\")\n",
    "\n",
    "# expire snapshots and remove manifest lists\n",
    "result_df = spark.sql(f\"\"\"\n",
    "CALL system.expire_snapshots(\n",
    "  table => 'lakekeeper.{catalog_namespace}.{iceberg_table_name}', \n",
    "  older_than => '2025-09-28 00:00:00'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:24:08.201726Z",
     "start_time": "2025-09-27T20:24:08.159406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " deleted_data_files_count            | 0   \n",
      " deleted_position_delete_files_count | 0   \n",
      " deleted_equality_delete_files_count | 0   \n",
      " deleted_manifest_files_count        | 0   \n",
      " deleted_manifest_lists_count        | 60  \n",
      " deleted_statistics_files_count      | 0   \n"
     ]
    }
   ],
   "source": [
    "result_df.show(1, 0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[83]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m result_df = spark.sql(\u001B[33mf\u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[33mCALL system.expire_snapshots(\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[33m  table => \u001B[39m\u001B[33m'\u001B[39m\u001B[33mlakekeeper.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcatalog_namespace\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00miceberg_table_name_hp\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\n\u001B[32m      4\u001B[39m \u001B[33m  older_than => \u001B[39m\u001B[33m'\u001B[39m\u001B[33m2025-09-28 00:00:00\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[33m)\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[33m\"\"\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mresults_df\u001B[49m.show(\u001B[32m1\u001B[39m, \u001B[32m0\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mNameError\u001B[39m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql(f\"\"\"\n",
    "CALL system.expire_snapshots(\n",
    "  table => 'lakekeeper.{catalog_namespace}.{iceberg_table_name_hp}', \n",
    "  older_than => '2025-09-28 00:00:00'\n",
    ")\n",
    "\"\"\")\n",
    "result_df.show(1, 0, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:25:10.531676Z",
     "start_time": "2025-09-27T20:25:09.311317Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " deleted_data_files_count            | 0   \n",
      " deleted_position_delete_files_count | 0   \n",
      " deleted_equality_delete_files_count | 0   \n",
      " deleted_manifest_files_count        | 0   \n",
      " deleted_manifest_lists_count        | 60  \n",
      " deleted_statistics_files_count      | 0   \n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql(f\"\"\"\n",
    "CALL system.expire_snapshots(\n",
    "  table => 'lakekeeper.{catalog_namespace}.{use_ddl_table}', \n",
    "  older_than => '2025-09-28 00:00:00'\n",
    ")\n",
    "\"\"\")\n",
    "result_df.show(1, 0, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T20:26:33.455888Z",
     "start_time": "2025-09-27T20:26:32.220672Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fruits of our Labor?\n",
    "Go ahead and **run the SQL query from before we tried to optimize the table** again. You may notice we're not getting a boost in speed. So what is happening here? \n",
    "\n",
    "The tables are actually performing really well. We don't have hundreds of gigabytes of metadata stacking up. The maintenance tasks so far are not a problem.\n",
    "\n",
    "* Next, we're going to move on and utilize our **Foundational Tables** as the basis for our Streaming Iceberg best practices. Follow along :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T01:00:08.615422Z",
     "start_time": "2025-09-27T01:00:08.607162Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# > Note: This fails with S3 IO.\n",
    "#clear_orphans_df = spark.sql(f\"\"\"\n",
    "#CALL system.remove_orphan_files(\n",
    "#  table => 'lakekeeper.{catalog_namespace}.{iceberg_table_name}',\n",
    "#  prefix_listing => false\n",
    "#)\n",
    "#\"\"\")\n",
    "# clear_orphans_df.show(1, 0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > Note: This is the nuclear option to scrap the table and all metadata and simply begin again!\n",
    "# spark.sql(f\"DROP TABLE {catalog_namespace}.{iceberg_table_name}\")\n",
    "#spark.sql(f\"DROP NAMESPACE {catalog_namespace}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
