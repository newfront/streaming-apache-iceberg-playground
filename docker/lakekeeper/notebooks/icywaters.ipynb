{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4931a60b-8def-469f-b8aa-fec1afd81146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T00:07:33.359782Z",
     "start_time": "2025-09-28T00:07:33.138586Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "# This CATALOG_URL works for the \"docker compose\" testing and development environment\n",
    "# Change 'lakekeeper' if you are not running on \"docker compose\" (f. ex. 'localhost' if Lakekeeper is running locally).\n",
    "CATALOG_URL = \"http://lakekeeper:8181/catalog\"\n",
    "WAREHOUSE = \"demo\"\n",
    "\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "SPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\n",
    "# Note: The driver memory should be within the bounds of the docker-compose.yaml 'services.jupyter.deploy.resources.[limits.memory,reservations.memory]'\n",
    "# jupyter will allocate the 'reservations' and attempt to apply up to the limit if possible.\n",
    "SPARK_DRIVER_MEMORY = '32G' # use total_docker_ram * .4\n",
    "HADOOP_AWS_VERSION = \"3.4.0\"\n",
    "ICEBERG_VERSION = \"1.10.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# fix local directory writes\n",
    "import os\n",
    "import stat\n",
    "\n",
    "# Create required directories\n",
    "directories = [\"/tmp/spark-local\", \"/tmp/s3a-buffer\", \"/tmp/spark-warehouse\"]\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, mode=0o777, exist_ok=True)\n",
    "    os.chmod(directory, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:02:49.364598Z",
     "start_time": "2025-09-28T00:02:49.361054Z"
    }
   },
   "id": "ea12d3008e9cac62"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e6a01cb-5caa-4220-aff7-d243f087c76a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T00:07:36.885240Z",
     "start_time": "2025-09-28T00:07:36.880114Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n",
    "    \"spark.sql.catalog.lakekeeper.uri\": CATALOG_URL,\n",
    "    \"spark.sql.catalog.lakekeeper.warehouse\": WAREHOUSE,\n",
    "    \"spark.sql.catalog.lakekeeper.io-impl\": \"org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n",
    "    \"spark.driver.memory\": SPARK_DRIVER_MEMORY,\n",
    "    \"spark.jars.packages\": f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.13:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION},org.apache.spark:spark-protobuf_2.13:{SPARK_VERSION},org.apache.hadoop:hadoop-aws:{HADOOP_AWS_VERSION}\",\n",
    "    # about zstd: Turn on zstd to make things highly compressed. Less size on disk, less IO bandwidth!\n",
    "    # we want to use zstd for parquet: \n",
    "    \"spark.sql.parquet.compression.codec\": \"zstd\",\n",
    "    \"spark.sql.iceberg.planning.preserve-data-grouping\": \"true\",\n",
    "    # we also want to use zstd for iceberg datafiles\n",
    "    \"spark.sql.iceberg.compression-codec\": \"zstd\",\n",
    "    \"spark.sql.iceberg.locality.enabled\": \"true\",\n",
    "    # note: merge-schema should be only set to true if you \"trust\" the upstream data producer\n",
    "    \"spark.sql.iceberg.merge-schema\": \"false\",\n",
    "    \"spark.hadoop.fs.s3.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "    \"spark.hadoop.fs.s3.endpoint\": \"http://minio:9000\",\n",
    "    \"spark.hadoop.fs.s3.access.key\": \"minio-root-user\",\n",
    "    \"spark.hadoop.fs.s3.secret.key\": \"minio-root-password\",\n",
    "    \"spark.hadoop.fs.s3.path.style.access\": \"true\",\n",
    "    \"spark.hadoop.fs.s3.connection.ssl.enabled\": \"false\",\n",
    "    \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "    \"spark.hadoop.fs.s3a.endpoint\": \"http://minio:9000\",\n",
    "    \"spark.hadoop.fs.s3a.access.key\": \"minio-root-user\",\n",
    "    \"spark.hadoop.fs.s3a.secret.key\": \"minio-root-password\",\n",
    "    \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n",
    "    \"spark.hadoop.fs.s3a.connection.ssl.enabled\": \"false\",\n",
    "    \"spark.hadoop.fs.s3a.committer.name\": \"magic\",\n",
    "    \"spark.hadoop.fs.s3a.committer.magic.enabled\": \"true\",\n",
    "    \"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\": \"true\",\n",
    "    \"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\": \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\",\n",
    "    \"spark.hadoop.fs.s3a.buffer.dir\": \"/tmp\",\n",
    "    \"spark.hadoop.fs.s3a.fast.upload\": \"true\",\n",
    "    \"spark.hadoop.fs.s3a.fast.upload.buffer\": \"disk\",\n",
    "    \"spark.local.dir\": \"/tmp/spark-local\",\n",
    "    \"spark.worker.dir\": \"/tmp/spark-worker\",\n",
    "    \"spark.sql.warehouse.dir\": \"/tmp/spark-warehouse\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01548ba5-29d5-40ef-86f3-9d461e9e08bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T00:07:42.588558Z",
     "start_time": "2025-09-28T00:07:39.119056Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_config = SparkConf().setMaster('local[*]').setAppName(\"Iceberg-REST\")\n",
    "for k, v in config.items():\n",
    "    spark_config = spark_config.set(k, v)\n",
    "\n",
    "spark: SparkSession = SparkSession.builder.config(conf=spark_config).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972eb547-9383-4706-8fe8-b7b6c0ee8956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T00:07:43.348444Z",
     "start_time": "2025-09-28T00:07:42.583649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE lakekeeper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[Database(name='icystreams', catalog='lakekeeper', description=None, locationUri='s3://examples/initial-warehouse/01998cb2-ac2e-7cb0-b341-ae5794445827')]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:07:46.156514Z",
     "start_time": "2025-09-28T00:07:45.536221Z"
    }
   },
   "id": "9f9324bd4949e008"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing shuffle partitions to 32\n"
     ]
    }
   ],
   "source": [
    "if int(spark.conf.get(\"spark.sql.shuffle.partitions\")) > 100:\n",
    "    print(\"reducing shuffle partitions to 32\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:07:47.275517Z",
     "start_time": "2025-09-28T00:07:47.271352Z"
    }
   },
   "id": "d33c08e9eb060076"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "16"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is set to the number of cores on the driver\n",
    "spark.sparkContext.defaultParallelism"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:07:47.699880Z",
     "start_time": "2025-09-28T00:07:47.694688Z"
    }
   },
   "id": "85cf155eae38d99f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# if this cell fails, you need to go through the exercises in \"first_steps-iceberg.ipynb\"\n",
    "catalog_namespace = 'icystreams'\n",
    "spark.catalog.setCurrentDatabase('icystreams')\n",
    "iceberg_table_name = 'ecomm'\n",
    "iceberg_table_name_hp = \"ecomm_hidden\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:07:48.499883Z",
     "start_time": "2025-09-28T00:07:48.482280Z"
    }
   },
   "id": "ee83b595401b842b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# uncomment and run to see what you have to work with. \n",
    "# use either of the two tables above for your streaming learning\n",
    "# spark.sql(f\"select count(*) as total from {catalog_namespace}.{iceberg_table_name}\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:03:02.586576Z",
     "start_time": "2025-09-28T00:03:02.578125Z"
    }
   },
   "id": "a13fcf45363ca476"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Find the earliest snapshot\n",
    "# if you blow away snapshots, then you can't really time travel anywhere :)\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import asc, desc\n",
    "\n",
    "def find_streaming_timestamp(namespace: str, table_name: str, from_earliest: bool = True) -> str :\n",
    "    \"\"\"\n",
    "    Helps to find the correct commit timestamp for the snapshot to stream from.\n",
    "    > Note: If you don't use the 'stream-from-timestamp' option, you will start streaming from 'latest' snapshot\n",
    "    > So us the flag `from_earliest` to flip this behavior (if you want to!) \n",
    "    :param namespace: The namespace of the table\n",
    "    :param table_name: The name of the table\n",
    "    :param from_earliest: specify which snapshot to pull from (earliest, or latest). Remember, the 'stream-from-timestamp' option defaults to 'latest' \n",
    "    :return: The commit timestamp for the snapshot to read from\n",
    "    \"\"\"\n",
    "    snapshots_df = (\n",
    "        spark.read\n",
    "        .table(f\"{namespace}.{table_name}.snapshots\")\n",
    "        .orderBy(asc('committed_at') if from_earliest else desc('committed_at'))\n",
    "    )\n",
    "    if snapshots_df.count() == 0:\n",
    "        raise ValueError(f\"No snapshots found for table {namespace}.{table_name}\")\n",
    "    return str(int(snapshots_df.head()['committed_at'].timestamp() * 1000)) # like: 1759003232707\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:07:53.254713Z",
     "start_time": "2025-09-28T00:07:53.249979Z"
    }
   },
   "id": "2410c008d6fc4e44"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1759003232707\n"
     ]
    }
   ],
   "source": [
    "print(find_streaming_timestamp(catalog_namespace, iceberg_table_name_hp))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:07:54.877994Z",
     "start_time": "2025-09-28T00:07:53.813844Z"
    }
   },
   "id": "9cfabd54e9449cee"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# create the streaming source dataframe.\n",
    "# we are going to set up a rate-limited stream\n",
    "# this is using 'streaming-max-files-per-micro-batch' and 'streaming-max-rows-per-micro-batch' to attempt to create a chunked stream\n",
    "# this will be used to \"manage\" reading torrents of data without the dreaded \"OutOfMemoryException\"'s we are so fond of\n",
    "\n",
    "# > Note: we know we have 109 million records, and each \"day\" contains over 100k records, so we will start ramping up the stream with \n",
    "# > a single file per micro-batch, and clamping at 10k records per trigger\n",
    "# > this is also a good strategy if you are looking to create \"wayyyy tooo much metadata!\" - you can play with the throttle\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "ecomm_streaming_source_df = (\n",
    "    spark.readStream\n",
    "    .format(\"iceberg\")\n",
    "    .option(\"streaming-max-files-per-micro-batch\", \"1\")\n",
    "    .option(\"streaming-max-rows-per-micro-batch\", \"10000\")\n",
    "    .option(\"streaming-skip-overwrite-snapshots\", \"true\")\n",
    "    .option(\"streaming-skip-delete-snapshots\", \"true\")\n",
    "    #.option(\"split-size\", str(2 * 1024 * 1024)) # control how we split files as we read them\n",
    "    .option(\"stream-from-timestamp\", find_streaming_timestamp(\n",
    "        catalog_namespace, \n",
    "        iceberg_table_name_hp)\n",
    "    ) # takes a str(long) for unix epoch milliseconds\n",
    "    .load(f\"{catalog_namespace}.{iceberg_table_name_hp}\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:07:57.374962Z",
     "start_time": "2025-09-28T00:07:57.164349Z"
    }
   },
   "id": "7ece32ddc1541c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "ecomm_streaming_source_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:07:59.169293Z",
     "start_time": "2025-09-28T00:07:59.161974Z"
    }
   },
   "id": "f18c9a441ea927ff"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- session_events: long (nullable = false)\n"
     ]
    }
   ],
   "source": [
    "# light transforms, cause why not!\n",
    "from pyspark.sql.functions import col, window, count\n",
    "\n",
    "transformed_df = (\n",
    "    ecomm_streaming_source_df\n",
    "    .select(\"event_time\", \"event_type\", \"product_id\", \"category_id\", \"price\", \"user_id\", \"user_session\")\n",
    "    .withWatermark(\"event_time\", \"30 minutes\")\n",
    "    .select(\"event_time\", \"event_type\", \"product_id\", \"user_session\", \"user_id\")\n",
    "    .groupBy(window(\"event_time\", \"30 minutes\"), \"user_id\", \"product_id\")\n",
    "    .agg(count(\"event_type\").alias('session_events'))\n",
    ")\n",
    "\n",
    "transformed_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:08:00.251914Z",
     "start_time": "2025-09-28T00:08:00.182851Z"
    }
   },
   "id": "994b89c90bff8ffd"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def table_exists(table_name: str):\n",
    "    return any(table.name == table_name for table in spark.catalog.listTables())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:08:01.451562Z",
     "start_time": "2025-09-28T00:08:01.444478Z"
    }
   },
   "id": "825d25a26071797f"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[Table(name='ecomm_hidden', catalog='lakekeeper', namespace=['icystreams'], description=None, tableType='MANAGED', isTemporary=False),\n Table(name='ecomm', catalog='lakekeeper', namespace=['icystreams'], description=None, tableType='MANAGED', isTemporary=False),\n Table(name='ecomm_hidden_other', catalog='lakekeeper', namespace=['icystreams'], description=None, tableType='MANAGED', isTemporary=False),\n Table(name='ecomm_hidden_aggregated', catalog='lakekeeper', namespace=['icystreams'], description=None, tableType='MANAGED', isTemporary=False),\n Table(name='ecomm_hidden_aggregated_new', catalog='lakekeeper', namespace=['icystreams'], description=None, tableType='MANAGED', isTemporary=False),\n Table(name='ecomm_hidden_aggregated_test', catalog='lakekeeper', namespace=['icystreams'], description=None, tableType='MANAGED', isTemporary=False)]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:08:02.689324Z",
     "start_time": "2025-09-28T00:08:02.434588Z"
    }
   },
   "id": "ed2cff1fd504fdde"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sink_table_name = f\"{iceberg_table_name_hp}_aggregated_other_again\"\n",
    "table_exists(sink_table_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:15:23.293649Z",
     "start_time": "2025-09-28T00:15:23.135979Z"
    }
   },
   "id": "5978c051b50f3f9a"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[]"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {catalog_namespace}.{sink_table_name} (\n",
    "        {transformed_df.schema.toDDL()}\n",
    "    ) USING iceberg\n",
    "    LOCATION 's3://examples/initial-warehouse/01998cb2-ac2e-7cb0-b341-ae5794445827/01998cb4-24f4-75d2-b67a-f9a39a1e30cc'\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:15:58.383898Z",
     "start_time": "2025-09-28T00:15:58.331062Z"
    }
   },
   "id": "1bec6192346918f5"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def get_table_location_from_describe(catalog_namespace, table_name):\n",
    "    \"\"\"\n",
    "    Extract location from DESCRIBE EXTENDED output\n",
    "    \"\"\"\n",
    "    table_df = spark.sql(f\"DESCRIBE EXTENDED {catalog_namespace}.{table_name}\")\n",
    "\n",
    "    # Convert to list of rows for easier processing\n",
    "    rows = table_df.collect()\n",
    "\n",
    "    # Look for the Location in the detailed table information\n",
    "    location = None\n",
    "    in_detailed_section = False\n",
    "\n",
    "    for row in rows:\n",
    "        col_name = row['col_name']\n",
    "        data_type = row['data_type']\n",
    "\n",
    "        # Check if we've reached the detailed section\n",
    "        if col_name == \"# Detailed Table Information\":\n",
    "            in_detailed_section = True\n",
    "            continue\n",
    "\n",
    "        # Look for Location in the detailed section\n",
    "        if in_detailed_section and col_name == \"Location\":\n",
    "            location = data_type\n",
    "            break\n",
    "\n",
    "    return location"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:08:08.080953Z",
     "start_time": "2025-09-28T00:08:08.076900Z"
    }
   },
   "id": "6eabbbafaaa856dc"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "#spark.sql(f\"\"\"\n",
    "#DESCRIBE TABLE EXTENDED {catalog_namespace}.{sink_table_name};\n",
    "#\"\"\").show(100, False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:08:08.838015Z",
     "start_time": "2025-09-28T00:08:08.830233Z"
    }
   },
   "id": "cb7eb83f89e321e"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# this is a protected table property, so we can ignore it\n",
    "#spark.sql(f\"\"\"\n",
    "#ALTER TABLE {catalog_namespace}.{sink_table_name} \n",
    "#SET TBLPROPERTIES ('owner' = 'jovyan'); \n",
    "#\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:08:09.254584Z",
     "start_time": "2025-09-28T00:08:09.250967Z"
    }
   },
   "id": "38db196dd51dafe8"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Setup the required configurations and directories to easily run our structured streaming application\n",
    "from pathlib import Path\n",
    "checkpoint_dir = Path('/home/jovyan').joinpath('checkpoints').absolute()\n",
    "\n",
    "app_name = 'streaming_ecomm_aggs_new'\n",
    "app_version = 'v1'\n",
    "\n",
    "# create a unique checkpoint directory so you can easily manage metadata for many tests/trials/etc\n",
    "# also: it is simple to overwrite other peoples checkpoints - so if you're using UC Volumes (then ownership is clutch there)\n",
    "app_checkpoint_dir = f\"s3a://examples/applications/{app_name}/{app_version}/_checkpoints\"\n",
    "#app_checkpoint_dir = checkpoint_dir.joinpath(app_name, app_version, \"_checkpoints\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:08:10.035275Z",
     "start_time": "2025-09-28T00:08:10.028702Z"
    }
   },
   "id": "1e5fbca99eac48bb"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'s3a://examples/applications/streaming_ecomm_aggs_new/v1/_checkpoints'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_checkpoint_dir"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:08:10.909853Z",
     "start_time": "2025-09-28T00:08:10.903108Z"
    }
   },
   "id": "7982fdce740df2fb"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "'ecomm_hidden_aggregated_test'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sink_table_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:08:11.343637Z",
     "start_time": "2025-09-28T00:08:11.336933Z"
    }
   },
   "id": "7a35834762ab5fab"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "#spark.sql(f\"drop table {catalog_namespace}.{sink_table_name}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:03:30.330486Z",
     "start_time": "2025-09-28T00:03:30.321913Z"
    }
   },
   "id": "57e77280fbd2937"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# setup the streaming aggregation\n",
    "spark.conf.set(\"spark.sql.iceberg.merge-schema\", \"true\")\n",
    "streaming_query: StreamingQuery = (\n",
    "    transformed_df\n",
    "    .writeStream\n",
    "    .format(\"iceberg\")\n",
    "    .queryName(\"ecomm_windowed_aggs\")\n",
    "    .option(\"checkpointLocation\", app_checkpoint_dir)\n",
    "    .option(\"fanout-enabled\", \"true\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(availableNow=True)  # 🚀 Goes as fast as possible while respecting your throttling limits!\n",
    "    .toTable(f\"lakekeeper.{catalog_namespace}.{sink_table_name}\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:18:14.379087Z",
     "start_time": "2025-09-28T00:18:14.199270Z"
    }
   },
   "id": "fa459de8c813d4ac"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "'s3://examples/initial-warehouse/01998cb2-ac2e-7cb0-b341-ae5794445827/01998d8d-50fa-7160-a3ba-be5698bb7e2b'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_table_location_from_describe(catalog_namespace, sink_table_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:03:46.977525Z",
     "start_time": "2025-09-28T00:03:46.922848Z"
    }
   },
   "id": "8e7d6ad330ecfea3"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def convert_s3_to_s3a(location):\n",
    "    \"\"\"\n",
    "    Convert s3:// URLs to s3a:// for Spark compatibility\n",
    "    \"\"\"\n",
    "    if location and location.startswith(\"s3://\"):\n",
    "        return location.replace(\"s3://\", \"s3a://\", 1)\n",
    "    return location\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:08:19.239772Z",
     "start_time": "2025-09-28T00:08:19.230729Z"
    }
   },
   "id": "eb08eaa236a06cfe"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/datasets/iceberg/ecomm_hidden_aggregated_test\n"
     ]
    }
   ],
   "source": [
    "print(f'/home/jovyan/datasets/iceberg/{sink_table_name}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:09:52.007141Z",
     "start_time": "2025-09-28T00:09:52.002561Z"
    }
   },
   "id": "bdb336394bb8f557"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o371.start.\n: org.apache.iceberg.exceptions.NoSuchTableException: Cannot find table for org.apache.iceberg.spark.PathIdentifier@55f29920.\n\tat org.apache.iceberg.spark.source.IcebergSource.getTable(IcebergSource.java:121)\n\tat org.apache.iceberg.spark.source.IcebergSource.inferPartitioning(IcebergSource.java:99)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:88)\n\tat org.apache.spark.sql.classic.DataStreamWriter.startInternal(DataStreamWriter.scala:280)\n\tat org.apache.spark.sql.classic.DataStreamWriter.start(DataStreamWriter.scala:136)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `s3://examples/initial-warehouse/01998cb2-ac2e-7cb0-b341-ae5794445827/01998cb4-24f4-75d2-b67a-f9a39a1e30cc`.`` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:173)\n\tat org.apache.iceberg.spark.source.IcebergSource.getTable(IcebergSource.java:116)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPy4JJavaError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[44]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      1\u001B[39m streaming_query: StreamingQuery = (\n\u001B[32m      2\u001B[39m     \u001B[43mtransformed_df\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mwriteStream\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43miceberg\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mqueryName\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mecomm_windowed_aggs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcheckpointLocation\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapp_checkpoint_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfanout-enabled\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtrue\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpath\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43ms3://examples/initial-warehouse/01998cb2-ac2e-7cb0-b341-ae5794445827/01998cb4-24f4-75d2-b67a-f9a39a1e30cc/\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43moutputMode\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcomplete\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrigger\u001B[49m\u001B[43m(\u001B[49m\u001B[43mavailableNow\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/spark/python/pyspark/sql/streaming/readwriter.py:1704\u001B[39m, in \u001B[36mDataStreamWriter.start\u001B[39m\u001B[34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[39m\n\u001B[32m   1702\u001B[39m     \u001B[38;5;28mself\u001B[39m.queryName(queryName)\n\u001B[32m   1703\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1704\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sq(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jwrite\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m   1705\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1706\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sq(\u001B[38;5;28mself\u001B[39m._jwrite.start(path))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1356\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1357\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1358\u001B[39m     args_command +\\\n\u001B[32m   1359\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1361\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1362\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1363\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1365\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1366\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:282\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    279\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpy4j\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mprotocol\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[32m    281\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m282\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    283\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    284\u001B[39m     converted = convert_exception(e.java_exception)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001B[39m, in \u001B[36mget_return_value\u001B[39m\u001B[34m(answer, gateway_client, target_id, name)\u001B[39m\n\u001B[32m    325\u001B[39m value = OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[32m2\u001B[39m:], gateway_client)\n\u001B[32m    326\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[32m1\u001B[39m] == REFERENCE_TYPE:\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[32m    328\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    329\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name), value)\n\u001B[32m    330\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    331\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[32m    332\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    333\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name, value))\n",
      "\u001B[31mPy4JJavaError\u001B[39m: An error occurred while calling o371.start.\n: org.apache.iceberg.exceptions.NoSuchTableException: Cannot find table for org.apache.iceberg.spark.PathIdentifier@55f29920.\n\tat org.apache.iceberg.spark.source.IcebergSource.getTable(IcebergSource.java:121)\n\tat org.apache.iceberg.spark.source.IcebergSource.inferPartitioning(IcebergSource.java:99)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:88)\n\tat org.apache.spark.sql.classic.DataStreamWriter.startInternal(DataStreamWriter.scala:280)\n\tat org.apache.spark.sql.classic.DataStreamWriter.start(DataStreamWriter.scala:136)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `s3://examples/initial-warehouse/01998cb2-ac2e-7cb0-b341-ae5794445827/01998cb4-24f4-75d2-b67a-f9a39a1e30cc`.`` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:173)\n\tat org.apache.iceberg.spark.source.IcebergSource.getTable(IcebergSource.java:116)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "#streaming_query: StreamingQuery = (\n",
    "#    transformed_df\n",
    "#    .writeStream\n",
    "#    .format(\"iceberg\")\n",
    "#    .queryName(\"ecomm_windowed_aggs\")\n",
    "#    .option(\"checkpointLocation\", app_checkpoint_dir)\n",
    "#    .option(\"fanout-enabled\", \"true\")\n",
    "#    .option(\"path\", f's3://examples/initial-warehouse/01998cb2-ac2e-7cb0-b341-ae5794445827/01998cb4-24f4-75d2-b67a-f9a39a1e30cc/')\n",
    "#    .outputMode(\"complete\")\n",
    "#    .trigger(availableNow=True)\n",
    "#    .start()\n",
    "#)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:17:26.561640Z",
     "start_time": "2025-09-28T00:17:26.469264Z"
    }
   },
   "id": "36e3c6942b5278f8"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_query.recentProgress"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:18:19.864398Z",
     "start_time": "2025-09-28T00:18:19.857145Z"
    }
   },
   "id": "d0e88ed474eb3f70"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "{'message': 'Terminated with exception: Forbidden: Table not found or action can_get_metadata forbidden for Anonymous',\n 'isDataAvailable': False,\n 'isTriggerActive': False}"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_query.status"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-28T00:18:20.979703Z",
     "start_time": "2025-09-28T00:18:20.973576Z"
    }
   },
   "id": "ba8fb7724be53293"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "be02e2a3b4528cdd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
